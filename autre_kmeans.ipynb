{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'instances présentes dans le corpus:  37 \n",
      "\n",
      "instance et taille des données d'entraînement [('aboutir', 25), ('investir', 25), ('traduire', 25), ('témoigner', 25), ('juger', 12), ('justifier', 25), ('viser', 25), ('prononcer', 25), ('accomplir', 25), ('convenir', 25), ('acquérir', 25), ('achever', 25), ('observer', 25), ('adapter', 25), ('admettre', 23), ('entraîner', 25), ('payer', 25), ('respecter', 24), ('affecter', 25), ('demeurer', 21), ('aggraver', 25), ('agir', 25), ('ajouter', 25), ('alimenter', 25), ('coûter', 22), ('relancer', 25), ('préférer', 25), ('appliquer', 25), ('apporter', 25), ('fonder', 25), ('appuyer', 25), ('changer', 22), ('chuter', 11), ('soutenir', 25), ('concevoir', 25), ('interroger', 25), ('confirmer', 25)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "creer_fichier_embeddings = False\n",
    "\n",
    "#1) Extraction des données d'entraînement\n",
    "\n",
    "#Eléments à définir\n",
    "data_path = \"FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "gold_path = \"FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "context_size = 4\n",
    "embeddings_path = \"embeddings.txt\" #Saisir le path du fichier existant ou le nom de celui qui sera crée dans le cas échéant\n",
    "\n",
    "#récupération des données XML\n",
    "tree = ET.parse(data_path)\n",
    "data_file = tree.getroot()[0]\n",
    "\n",
    "#récupération des données .txt\n",
    "gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "\n",
    "def extract_examples_and_senses(data_file, gold_file):\n",
    "    \"\"\"Extract the data from the files.\n",
    "\n",
    "    Args:\n",
    "        data_file (Element): Sentences\n",
    "        gold_file (TextIOWrapper): Golds keys\n",
    "\n",
    "    Returns:\n",
    "        dictionary: associates the list of context vectors corresponding to the instance\n",
    "        dictionary : associates to the word each senses\n",
    "    \"\"\"\n",
    "    \n",
    "    w2examples={}\n",
    "    w2senses = defaultdict(set)\n",
    "    \n",
    "    for (sentence,gold_line) in zip(data_file,gold_file.readlines()) :\n",
    "        \n",
    "        #pour chaque phrase, on initialise deux listes qui permettront de respecter les tailles des contextes (+10,-10)\n",
    "        context_before = []\n",
    "        context_after = []\n",
    "        context = []\n",
    "        \n",
    "        #on boucle sur les mots de la phrase pour construire les listes\n",
    "        #on cherche l'instance et on repart en arrière pour constuire le contexte avant\n",
    "        i_instance = 0\n",
    "        while sentence[i_instance].tag != \"instance\" : \n",
    "            i_instance+=1\n",
    "        \n",
    "        instance = sentence[i_instance].attrib[\"lemma\"].lower()\n",
    "        \n",
    "        if instance not in w2examples : \n",
    "            w2examples[instance] = []\n",
    "        \n",
    "        #on vérifie la longueur des phrases pour ne pas soulever d'erreur\n",
    "        \n",
    "        #context_before \n",
    "        \n",
    "        #si le contexte avant l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "        #on ajoute à la liste chaque mot aux index from i-instance-1 to i_instance-5\n",
    "        if (len(sentence[:i_instance])>=context_size) :\n",
    "                for i in range(1,context_size+1) :\n",
    "                    context_before.append(sentence[i_instance-i].text.lower())\n",
    "        \n",
    "        #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "        else :\n",
    "            for i in range(1,len(sentence[:i_instance])+1) :\n",
    "                context_before.append(sentence[i_instance-i].text.lower())\n",
    "\n",
    "        #context_after\n",
    "        \n",
    "        #si le contexte après l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "        #on ajoute à la liste chaque mot aux index from i-instance+1 to i_instance+11\n",
    "        if(len(sentence[i_instance+1:])>= context_size) :\n",
    "            for i in range(i_instance+1,i_instance+(context_size+1)):\n",
    "                context_after.append(sentence[i].text.lower())\n",
    "        \n",
    "        #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "        else :\n",
    "            for i in range(i_instance+1,len(sentence)):\n",
    "                context_after.append(sentence[i].text.lower())\n",
    "        \n",
    "        #une fois les listes constituées, on ajoute les balises de début et de fin de phrase si nécessaire\n",
    "        for i in range(context_size-len(context_before)) :\n",
    "            context_before.append(\"<d>\")\n",
    "            \n",
    "        for i in range(context_size-len(context_after)) :\n",
    "            context_after.append(\"<f>\")\n",
    "            \n",
    "        #le vecteur sera une concaténation des contextes d'avant et d'après\n",
    "        context = context_before\n",
    "        context.append(instance)\n",
    "        context.extend(context_after)\n",
    "            \n",
    "        #on récupère ensuite le nombre associé au sens pour constuire l'exemple + ajouter au dictionnaire w2sense\n",
    "        gold = int((re.findall(\"ws_[0-9]\",gold_line)[0]).replace(\"ws_\",\"\"))\n",
    "        \n",
    "        w2senses[instance].add(gold)\n",
    "        w2examples[instance].append((context,gold))\n",
    "        \n",
    "    return w2examples,w2senses\n",
    "\n",
    "w2examples,w2senses = extract_examples_and_senses(data_file,gold_file)\n",
    "instances = list(w2examples.keys())\n",
    "print(\"nombre d'instances présentes dans le corpus: \",len(instances),\"\\n\")\n",
    "print(\"instance et taille des données d'entraînement\",[(word,len(w2examples[word])) for word in w2examples],\"\\n\")\n",
    "\n",
    "#2) Facultatif : script pour créer le fichier texte ne comportant que les embeddings qui nous intéressent - plus rapide\n",
    "\n",
    "if creer_fichier_embeddings :\n",
    "    #Element à définir\n",
    "    fasstex_path = \"../fasstex\" \n",
    "\n",
    "    #création du vocabulaire du corpus entier\n",
    "    i2w = set()\n",
    "    for instance,examples in w2examples.items():\n",
    "        for context,gold in examples :\n",
    "            i2w.update(context)\n",
    "    i2w = list(i2w)\n",
    "\n",
    "    f = open(fasstex_path, \"r\", encoding=\"UTF-8\")\n",
    "    f.readline() #permet de ne pas prendre en compte la première ligne du fichier qui résumé ce que contient le fichier\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(embeddings_path,\"w\",encoding=\"UTF-8\") as f : \n",
    "            f.writelines([line for line in lines if line.split(\" \")[0] in i2w])\n",
    "\n",
    "#3) Construction du dictionnaire qui va nous permettre de faire le look up (matrice d'embeddings)\n",
    "\n",
    "def extract_embeddings(path_embeddings) :\n",
    "    \"\"\"Récupère les embeddings dans le fichier générée.\n",
    "\n",
    "    Args:\n",
    "        path_embeddings (string)\n",
    "\n",
    "    Returns:\n",
    "        dictionnary: Associe à chaque mot son embedding\n",
    "    \"\"\"\n",
    "    f = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    #On récupère dans le fichier crée les embeddings pour créer un dictionnaire\n",
    "    w2emb = {}\n",
    "    for line in f.readlines():\n",
    "        splitted_line = line.split(\" \")\n",
    "        word = splitted_line[0]\n",
    "        embedding = list(map(float,splitted_line[1:]))\n",
    "        w2emb[word] = embedding\n",
    "    return w2emb\n",
    "\n",
    "w2emb = extract_embeddings(embeddings_path)\n",
    "#4) Sélection des instances pour la classification et opération de look up pour chacune d'elle\n",
    "\n",
    "#Elément à définir #EST-CE QUE LA ON MET LA FONCTION MAKE_BATCHES ?\n",
    "instances_to_test = instances[:3]\n",
    "\n",
    "\n",
    "def look_up(context, w2emb) :\n",
    "    \"\"\"Remplace dans le vecteur de contexte les mots par leur embedding.\n",
    "\n",
    "    Args:\n",
    "        context (list): liste de taille (size_window*2)+1\n",
    "        w2emb (dictionnary): Associe à chaque mot son embedding\n",
    "\n",
    "    Returns:\n",
    "        list : liste de taille size_embedding : BOW\n",
    "    \"\"\"\n",
    "    emb_size = len(list(w2emb.values())[0]) #on récupère la taille d'un embedding : 300\n",
    "    context_emb = np.zeros(emb_size)\n",
    "    for word in context :\n",
    "        if word in w2emb :\n",
    "            context_emb = np.add(context_emb, np.array(w2emb[word]))             \n",
    "    return context_emb\n",
    "\n",
    "#5) Sélection de données représentatives du corpus d'entraînement en fonction du nombre de données considérées\n",
    "size_data = [1,0.8,0.6,0.4,0.2]\n",
    "#Remarque : \"affecter\" compte 7 senses différents pour 25 examples et \"demeurer\",6 senses pour 21 examples.\n",
    "#Conséquence : on ne peut pas considérer moins de 25% des données annotées pour \"affecter\" et 30% pour \"demeurer\"\n",
    "\n",
    "def select_examples(examples,senses,size):\n",
    "    \"\"\"Choisit des examples d'entraînement représentatifs du corpus.\n",
    "\n",
    "    Args:\n",
    "        examples (list)\n",
    "        senses (int): nombre de senses associés à l'instance\n",
    "        size (float): quantité des données d'entraînement considérés\n",
    "\n",
    "    Returns:\n",
    "        list: examples qui contiennent au moins un example de chaque sense\n",
    "    \"\"\"\n",
    "    selected_examples = []\n",
    "    \n",
    "    #Pour chaque sens, on ajoute un example associé à ce sens ,au hasard\n",
    "    for sense in senses :\n",
    "        selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),examples))))\n",
    "    \n",
    "    #On calcule ensuite le nombre d'examples qu'il reste à ajouter pour atteindre la quantité de données souhaitée\n",
    "    size_to_add = round(size*(len(examples)))-len(selected_examples)\n",
    "    \n",
    "    #On ajoute ce nombre de données (non-présentes déjà dans la liste) selectionnées au hasard\n",
    "    selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),examples)),k=size_to_add))\n",
    "    \n",
    "    return selected_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotated_examples, not_annotated_examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "                    si example = ([1.9, 2.3, 0.6], 1),\n",
    "                    - le contexte avec le mot à désambiguiser et son lemme est \n",
    "                      l'embedding [1.9, 2.3, 0.6]\n",
    "                    - le numéro de sens est 1\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.annotated_examples = annotated_examples\n",
    "\n",
    "        # on a le gold, mais on ne l'utilisera que pour calculer l'accuracy\n",
    "        # pour le training, nous ferons comme si nous n'avions pas la gold class\n",
    "        self.not_annotated_examples = not_annotated_examples\n",
    "\n",
    "        # transforme les embeddings en tensors\n",
    "        # ce sont les exemples qui devront être classifiés\n",
    "        self.tensors_examples = [example[0] for example in self.not_annotated_examples]\n",
    "\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données annotées, qui représentent tous les sens possibles\n",
    "        self.k = len(set([example[1] for example in self.annotated_examples]))\n",
    "\n",
    "        \n",
    "        # initialisation des centroïdes : pour chaque sens, le centroïde \n",
    "        # correspond à la moyenne des embeddings des exemples annotés\n",
    "        # Ainsi, chaque centroïde représente un sens\n",
    "        self.tensors_centroids, self.cluster2sense = self.make_centroids()\n",
    "\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(not_annotated_examples))\n",
    "\n",
    "\n",
    "\n",
    "    def make_centroids(self):\n",
    "        cluster2sense = []\n",
    "        tensors_centroids = []\n",
    "        senses = set([example[1] for example in self.annotated_examples])\n",
    "        for sense in senses: \n",
    "            # on récupère les examples du sens\n",
    "            examples_sense = [example[0] for example in self.annotated_examples if example[1] == sense]\n",
    "            # on calcule le centroid du sens\n",
    "            centroid = torch.mean(torch.stack(examples_sense), dim=0)\n",
    "            # on ajoute le centroid à la liste des centroids\n",
    "            tensors_centroids.append(centroid)\n",
    "            # on ajoute le sens à la liste des sens\n",
    "            #L'index du sens correspond au numéro du cluster\n",
    "            cluster2sense.append(sense)\n",
    "\n",
    "        return tensors_centroids, cluster2sense\n",
    "\n",
    "\n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        while diff:\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "\n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = (tensor_centroid - tensor_example).pow(2).sum(axis=0).sqrt()\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "            # CALCUL DES NOUVEAUX CENTROIDES\n",
    "\n",
    "            # calcul des nouveaux centroides en utilisant le point au milieu de tous les\n",
    "            # autres points du même cluster\n",
    "            new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean()\n",
    "            # transforme ces nouveaux centroides en tensors\n",
    "            tensors_new_centroids = []\n",
    "            for i in range(len(new_centroids.index)):\n",
    "                colums = []\n",
    "                for j in range(len(new_centroids.columns)):\n",
    "                    colums.append(int(new_centroids.iat[i,j]))\n",
    "                tensors_new_centroids.append(torch.tensor(colums))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "            '''J'ai essayé de régler le problème de range mais possible que yen ait tjrs '''\n",
    "            count_diff = 0\n",
    "            for i in range(len(tensors_new_centroids)):\n",
    "                if not torch.equal(self.tensors_centroids[i], tensors_new_centroids[i]):\n",
    "                    count_diff += 1\n",
    "                    self.tensors_centroids[i] = tensors_new_centroids[i]\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            \n",
    "            ''' Ancien bout de code de Mathilde\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            '''\n",
    "            \n",
    "\n",
    "    def accuracy(self): \n",
    "        correct = 0\n",
    "        for i in range(len(self.not_annotated_examples)):\n",
    "            sens_attendu = self.not_annotated_examples[i][1]\n",
    "            sens_assigne = self.cluster2sense[int(self.clusters[i])]\n",
    "            #print(sens_assigne, sens_attendu)\n",
    "            if sens_attendu == sens_assigne:\n",
    "                correct += 1\n",
    "        return correct, len(self.not_annotated_examples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% de données annotées\n",
      "--> Instance : aboutir\n",
      "Le sens le plus fréquent pour 'aboutir' est le sens 3 avec une proportion de 99.69 %\n",
      "Avec le kmeans et 100% des données annotées pour 'aboutir', l'accuracy est de 0.0 %\n",
      "\n",
      "--> Instance : investir\n",
      "Le sens le plus fréquent pour 'investir' est le sens 3 avec une proportion de 61.9 %\n",
      "Avec le kmeans et 100% des données annotées pour 'investir', l'accuracy est de 12.45 %\n",
      "\n",
      "--> Instance : traduire\n",
      "Le sens le plus fréquent pour 'traduire' est le sens 1 avec une proportion de 80.55 %\n",
      "Avec le kmeans et 100% des données annotées pour 'traduire', l'accuracy est de 20.85 %\n",
      "\n",
      "Micro-average : 9.96 %\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "80.0% de données annotées\n",
      "--> Instance : aboutir\n",
      "Le sens le plus fréquent pour 'aboutir' est le sens 3 avec une proportion de 99.73 %\n",
      "Avec le kmeans et 80.0% des données annotées pour 'aboutir', l'accuracy est de 0.0 %\n",
      "\n",
      "--> Instance : investir\n",
      "Le sens le plus fréquent pour 'investir' est le sens 4 avec une proportion de 50.0 %\n",
      "Avec le kmeans et 80.0% des données annotées pour 'investir', l'accuracy est de 16.75 %\n",
      "\n",
      "--> Instance : traduire\n",
      "Le sens le plus fréquent pour 'traduire' est le sens 1 avec une proportion de 99.21 %\n",
      "Avec le kmeans et 80.0% des données annotées pour 'traduire', l'accuracy est de 15.52 %\n",
      "\n",
      "Micro-average : 9.65 %\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "60.0% de données annotées\n",
      "--> Instance : aboutir\n",
      "Le sens le plus fréquent pour 'aboutir' est le sens 3 avec une proportion de 99.81 %\n",
      "Avec le kmeans et 60.0% des données annotées pour 'aboutir', l'accuracy est de 0.0 %\n",
      "\n",
      "--> Instance : investir\n",
      "Le sens le plus fréquent pour 'investir' est le sens 4 avec une proportion de 50.03 %\n",
      "Avec le kmeans et 60.0% des données annotées pour 'investir', l'accuracy est de 16.1 %\n",
      "\n",
      "--> Instance : traduire\n",
      "Le sens le plus fréquent pour 'traduire' est le sens 1 avec une proportion de 99.52 %\n",
      "Avec le kmeans et 60.0% des données annotées pour 'traduire', l'accuracy est de 83.64 %\n",
      "\n",
      "Micro-average : 21.46 %\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "40.0% de données annotées\n",
      "--> Instance : aboutir\n",
      "Le sens le plus fréquent pour 'aboutir' est le sens 3 avec une proportion de 99.82 %\n",
      "Avec le kmeans et 40.0% des données annotées pour 'aboutir', l'accuracy est de 0.0 %\n",
      "\n",
      "--> Instance : investir\n",
      "Le sens le plus fréquent pour 'investir' est le sens 4 avec une proportion de 54.72 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1490\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49m_cython_operation(\n\u001b[0;32m   1491\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39maggregate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m         values,\n\u001b[0;32m   1493\u001b[0m         how,\n\u001b[0;32m   1494\u001b[0m         axis\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1495\u001b[0m         min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   1496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1497\u001b[0m     )\n\u001b[0;32m   1498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:959\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups\n\u001b[1;32m--> 959\u001b[0m \u001b[39mreturn\u001b[39;00m cy_op\u001b[39m.\u001b[39;49mcython_operation(\n\u001b[0;32m    960\u001b[0m     values\u001b[39m=\u001b[39;49mvalues,\n\u001b[0;32m    961\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    962\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    963\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mids,\n\u001b[0;32m    964\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    965\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    966\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:657\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[1;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[0;32m    650\u001b[0m         values,\n\u001b[0;32m    651\u001b[0m         min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    655\u001b[0m     )\n\u001b[1;32m--> 657\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_op_ndim_compat(\n\u001b[0;32m    658\u001b[0m     values,\n\u001b[0;32m    659\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    660\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    661\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[0;32m    662\u001b[0m     mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    663\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    664\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:497\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mT\n\u001b[1;32m--> 497\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_cython_op(\n\u001b[0;32m    498\u001b[0m     values,\n\u001b[0;32m    499\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m    500\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[0;32m    501\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[0;32m    502\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[0;32m    503\u001b[0m     result_mask\u001b[39m=\u001b[39;49mresult_mask,\n\u001b[0;32m    504\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    505\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:541\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m out_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_shape(ngroups, values)\n\u001b[1;32m--> 541\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_cython_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkind, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow, values\u001b[39m.\u001b[39;49mdtype, is_numeric)\n\u001b[0;32m    542\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_cython_vals(values)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:173\u001b[0m, in \u001b[0;36mWrappedCythonOp._get_cython_function\u001b[1;34m(cls, kind, how, dtype, is_numeric)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39m__signatures__:\n\u001b[0;32m    172\u001b[0m     \u001b[39m# raise NotImplementedError here rather than TypeError later\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    174\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfunction is not implemented for this dtype: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[how->\u001b[39m\u001b[39m{\u001b[39;00mhow\u001b[39m}\u001b[39;00m\u001b[39m,dtype->\u001b[39m\u001b[39m{\u001b[39;00mdtype_str\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: function is not implemented for this dtype: [how->mean,dtype->object]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m not_annotated_examples \u001b[39m=\u001b[39m [(X[i],y_gold[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))]\n\u001b[0;32m     35\u001b[0m k_Means \u001b[39m=\u001b[39m K_Means(annotated_examples, not_annotated_examples)\n\u001b[1;32m---> 37\u001b[0m k_Means\u001b[39m.\u001b[39;49mlearn_clusters()\n\u001b[0;32m     38\u001b[0m num_correct, num_examples \u001b[39m=\u001b[39m k_Means\u001b[39m.\u001b[39maccuracy()\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvec le kmeans et \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% des données annotées pour \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minstance\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, l\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy est de \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(num_correct\u001b[39m/\u001b[39mnum_examples\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m,\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[140], line 105\u001b[0m, in \u001b[0;36mK_Means.learn_clusters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclusters[i] \u001b[39m=\u001b[39m j\n\u001b[0;32m    101\u001b[0m \u001b[39m# CALCUL DES NOUVEAUX CENTROIDES\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[39m# calcul des nouveaux centroides en utilisant le point au milieu de tous les\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m# autres points du même cluster\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m new_centroids \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors_examples)\u001b[39m.\u001b[39;49mgroupby(by \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclusters)\u001b[39m.\u001b[39;49mmean()\n\u001b[0;32m    106\u001b[0m \u001b[39m# transforme ces nouveaux centroides en tensors\u001b[39;00m\n\u001b[0;32m    107\u001b[0m tensors_new_centroids \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1855\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[1;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[0;32m   1854\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1855\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[0;32m   1856\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1857\u001b[0m         alt\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[0;32m   1858\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[0;32m   1859\u001b[0m     )\n\u001b[0;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1507\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim, alt\u001b[39m=\u001b[39malt)\n\u001b[0;32m   1505\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1507\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgrouped_reduce(array_func)\n\u001b[0;32m   1508\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[0;32m   1509\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1503\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[39mif\u001b[39;00m blk\u001b[39m.\u001b[39mis_object:\n\u001b[0;32m   1500\u001b[0m     \u001b[39m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m     \u001b[39m#  while others do not.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m     \u001b[39mfor\u001b[39;00m sb \u001b[39min\u001b[39;00m blk\u001b[39m.\u001b[39m_split():\n\u001b[1;32m-> 1503\u001b[0m         applied \u001b[39m=\u001b[39m sb\u001b[39m.\u001b[39;49mapply(func)\n\u001b[0;32m   1504\u001b[0m         result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m   1505\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Block]:\n\u001b[0;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    one\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    331\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1503\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39m_cython_operation(\n\u001b[0;32m   1491\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1492\u001b[0m         values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1497\u001b[0m     )\n\u001b[0;32m   1498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[1;32m-> 1503\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim, alt\u001b[39m=\u001b[39;49malt)\n\u001b[0;32m   1505\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1457\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[1;34m(self, values, ndim, alt)\u001b[0m\n\u001b[0;32m   1452\u001b[0m     ser \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m   1454\u001b[0m \u001b[39m# We do not get here with UDFs, so we know that our dtype\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[39m#  should always be preserved by the implemented aggregations\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[39m# TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\u001b[39;00m\n\u001b[1;32m-> 1457\u001b[0m res_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49magg_series(ser, alt, preserve_dtype\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, Categorical):\n\u001b[0;32m   1460\u001b[0m     \u001b[39m# Because we only get here with known dtype-preserving\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m     \u001b[39m#  reductions, we cast back to Categorical.\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m     \u001b[39m# TODO: if we ever get \"rank\" working, exclude it here.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m     res_values \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(values)\u001b[39m.\u001b[39m_from_sequence(res_values, dtype\u001b[39m=\u001b[39mvalues\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:994\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39m_values, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    988\u001b[0m     \u001b[39m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[39m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[39m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     \u001b[39m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 994\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[0;32m    996\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    997\u001b[0m \u001b[39mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:1015\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   1012\u001b[0m splitter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_splitter(obj, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m   1014\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[1;32m-> 1015\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[0;32m   1016\u001b[0m     res \u001b[39m=\u001b[39m libreduction\u001b[39m.\u001b[39mextract_result(res)\n\u001b[0;32m   1018\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m initialized:\n\u001b[0;32m   1019\u001b[0m         \u001b[39m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1857\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[0;32m   1854\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1855\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   1856\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m-> 1857\u001b[0m         alt\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[0;32m   1858\u001b[0m         numeric_only\u001b[39m=\u001b[39mnumeric_only,\n\u001b[0;32m   1859\u001b[0m     )\n\u001b[0;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:11563\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11546\u001b[0m \u001b[39m@doc\u001b[39m(\n\u001b[0;32m  11547\u001b[0m     _num_doc,\n\u001b[0;32m  11548\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReturn the mean of the values over the requested axis.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11561\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11562\u001b[0m ):\n\u001b[1;32m> 11563\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39;49mmean(\u001b[39mself\u001b[39;49m, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:11208\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[0;32m  11202\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  11203\u001b[0m     axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11206\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m> 11208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stat_function(\n\u001b[0;32m  11209\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, nanops\u001b[39m.\u001b[39;49mnanmean, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m  11210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:11165\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11161\u001b[0m     nv\u001b[39m.\u001b[39mvalidate_stat_func((), kwargs, fname\u001b[39m=\u001b[39mname)\n\u001b[0;32m  11163\u001b[0m validate_bool_kwarg(skipna, \u001b[39m\"\u001b[39m\u001b[39mskipna\u001b[39m\u001b[39m\"\u001b[39m, none_allowed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m> 11165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reduce(\n\u001b[0;32m  11166\u001b[0m     func, name\u001b[39m=\u001b[39;49mname, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49mnumeric_only\n\u001b[0;32m  11167\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4671\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   4667\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSeries.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not allow \u001b[39m\u001b[39m{\u001b[39;00mkwd_name\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mnumeric_only\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4668\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith non-numeric dtypes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4669\u001b[0m     )\n\u001b[0;32m   4670\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 4671\u001b[0m     \u001b[39mreturn\u001b[39;00m op(delegate, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m is_object_dtype(args[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    156\u001b[0m         result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:421\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mif\u001b[39;00m datetimelike \u001b[39mand\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[1;32m--> 421\u001b[0m result \u001b[39m=\u001b[39m func(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, mask\u001b[39m=\u001b[39;49mmask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    423\u001b[0m \u001b[39mif\u001b[39;00m datetimelike:\n\u001b[0;32m    424\u001b[0m     result \u001b[39m=\u001b[39m _wrap_results(result, orig_values\u001b[39m.\u001b[39mdtype, fill_value\u001b[39m=\u001b[39miNaT)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:727\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    724\u001b[0m     dtype_count \u001b[39m=\u001b[39m dtype\n\u001b[0;32m    726\u001b[0m count \u001b[39m=\u001b[39m _get_counts(values\u001b[39m.\u001b[39mshape, mask, axis, dtype\u001b[39m=\u001b[39mdtype_count)\n\u001b[1;32m--> 727\u001b[0m the_sum \u001b[39m=\u001b[39m _ensure_numeric(values\u001b[39m.\u001b[39;49msum(axis, dtype\u001b[39m=\u001b[39;49mdtype_sum))\n\u001b[0;32m    729\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(the_sum, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    730\u001b[0m     count \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, count)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for size in size_data :\n",
    "\n",
    "    df_size_data = []\n",
    "\n",
    "    list_num_correct = []\n",
    "    list_num_examples = []\n",
    "    \n",
    "    print(f'{size*100}% de données annotées')\n",
    "\n",
    "    for instance in instances_to_test :\n",
    "            \n",
    "        print(\"--> Instance :\",instance)\n",
    "        \n",
    "        annotated_examples = select_examples(w2examples[instance],w2senses[instance],size)\n",
    "\n",
    "        not_annotated_examples = [ex for ex in select_examples(w2examples[instance],w2senses[instance],100) if ex not in annotated_examples]\n",
    "\n",
    "        \n",
    "        \n",
    "        X_annotated = [torch.from_numpy(look_up(context,w2emb)) for context,gold in annotated_examples]\n",
    "        y_annotated = [gold for context,gold in annotated_examples]\n",
    "\n",
    "        X = [torch.from_numpy(look_up(context,w2emb)) for context,gold in not_annotated_examples]\n",
    "        y_gold = [gold for context,gold in not_annotated_examples]\n",
    "        \n",
    "        y = y_annotated + y_gold\n",
    "        #print(y)\n",
    "        most_frequent_sense = max(y,key=y.count)\n",
    "        occurrence_of_most_frequent_sense = y.count(most_frequent_sense)/len(y)\n",
    "        print(f\"Le sens le plus fréquent pour '{instance}' est le sens {most_frequent_sense} avec une proportion de {round(occurrence_of_most_frequent_sense*100,2)} %\")\n",
    "        \n",
    "        annotated_examples = [(X_annotated[i],y_annotated[i]) for i in range(len(X_annotated))]\n",
    "        not_annotated_examples = [(X[i],y_gold[i]) for i in range(len(X))]\n",
    "        k_Means = K_Means(annotated_examples, not_annotated_examples)\n",
    "\n",
    "        k_Means.learn_clusters()\n",
    "        num_correct, num_examples = k_Means.accuracy()\n",
    "        print(f\"Avec le kmeans et {size*100}% des données annotées pour '{instance}', l'accuracy est de {round(num_correct/num_examples*100,2)} %\")\n",
    "        print()\n",
    "        \n",
    "        list_num_correct.append(num_correct)\n",
    "        list_num_examples.append(num_examples)\n",
    "        \n",
    "    micro_average = sum(list_num_correct)/sum(list_num_examples)\n",
    "    print(f\"Micro-average : {round(micro_average*100,2)} %\")\n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
