{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'instances présentes dans le corpus:  37 \n",
      "\n",
      "instance et taille des données d'entraînement [('aboutir', 25), ('investir', 25), ('traduire', 25), ('témoigner', 25), ('juger', 12), ('justifier', 25), ('viser', 25), ('prononcer', 25), ('accomplir', 25), ('convenir', 25), ('acquérir', 25), ('achever', 25), ('observer', 25), ('adapter', 25), ('admettre', 23), ('entraîner', 25), ('payer', 25), ('respecter', 24), ('affecter', 25), ('demeurer', 21), ('aggraver', 25), ('agir', 25), ('ajouter', 25), ('alimenter', 25), ('coûter', 22), ('relancer', 25), ('préférer', 25), ('appliquer', 25), ('apporter', 25), ('fonder', 25), ('appuyer', 25), ('changer', 22), ('chuter', 11), ('soutenir', 25), ('concevoir', 25), ('interroger', 25), ('confirmer', 25)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "creer_fichier_embeddings = False\n",
    "\n",
    "#1) Extraction des données d'entraînement\n",
    "\n",
    "#Eléments à définir\n",
    "data_path = \"FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "gold_path = \"FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "context_size = 4\n",
    "embeddings_path = \"embeddings.txt\" #Saisir le path du fichier existant ou le nom de celui qui sera crée dans le cas échéant\n",
    "\n",
    "#récupération des données XML\n",
    "tree = ET.parse(data_path)\n",
    "data_file = tree.getroot()[0]\n",
    "\n",
    "#récupération des données .txt\n",
    "gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "\n",
    "def extract_examples_and_senses(data_file, gold_file):\n",
    "    \"\"\"Extract the data from the files.\n",
    "\n",
    "    Args:\n",
    "        data_file (Element): Sentences\n",
    "        gold_file (TextIOWrapper): Golds keys\n",
    "\n",
    "    Returns:\n",
    "        dictionary: associates the list of context vectors corresponding to the instance\n",
    "        dictionary : associates to the word each senses\n",
    "    \"\"\"\n",
    "    \n",
    "    w2examples={}\n",
    "    w2senses = defaultdict(set)\n",
    "    \n",
    "    for (sentence,gold_line) in zip(data_file,gold_file.readlines()) :\n",
    "        \n",
    "        #pour chaque phrase, on initialise deux listes qui permettront de respecter les tailles des contextes (+10,-10)\n",
    "        context_before = []\n",
    "        context_after = []\n",
    "        context = []\n",
    "        \n",
    "        #on boucle sur les mots de la phrase pour construire les listes\n",
    "        #on cherche l'instance et on repart en arrière pour constuire le contexte avant\n",
    "        i_instance = 0\n",
    "        while sentence[i_instance].tag != \"instance\" : \n",
    "            i_instance+=1\n",
    "        \n",
    "        instance = sentence[i_instance].attrib[\"lemma\"].lower()\n",
    "        \n",
    "        if instance not in w2examples : \n",
    "            w2examples[instance] = []\n",
    "        \n",
    "        #on vérifie la longueur des phrases pour ne pas soulever d'erreur\n",
    "        \n",
    "        #context_before \n",
    "        \n",
    "        #si le contexte avant l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "        #on ajoute à la liste chaque mot aux index from i-instance-1 to i_instance-5\n",
    "        if (len(sentence[:i_instance])>=context_size) :\n",
    "                for i in range(1,context_size+1) :\n",
    "                    context_before.append(sentence[i_instance-i].text.lower())\n",
    "        \n",
    "        #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "        else :\n",
    "            for i in range(1,len(sentence[:i_instance])+1) :\n",
    "                context_before.append(sentence[i_instance-i].text.lower())\n",
    "\n",
    "        #context_after\n",
    "        \n",
    "        #si le contexte après l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "        #on ajoute à la liste chaque mot aux index from i-instance+1 to i_instance+11\n",
    "        if(len(sentence[i_instance+1:])>= context_size) :\n",
    "            for i in range(i_instance+1,i_instance+(context_size+1)):\n",
    "                context_after.append(sentence[i].text.lower())\n",
    "        \n",
    "        #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "        else :\n",
    "            for i in range(i_instance+1,len(sentence)):\n",
    "                context_after.append(sentence[i].text.lower())\n",
    "        \n",
    "        #une fois les listes constituées, on ajoute les balises de début et de fin de phrase si nécessaire\n",
    "        for i in range(context_size-len(context_before)) :\n",
    "            context_before.append(\"<d>\")\n",
    "            \n",
    "        for i in range(context_size-len(context_after)) :\n",
    "            context_after.append(\"<f>\")\n",
    "            \n",
    "        #le vecteur sera une concaténation des contextes d'avant et d'après\n",
    "        context = context_before\n",
    "        context.append(instance)\n",
    "        context.extend(context_after)\n",
    "            \n",
    "        #on récupère ensuite le nombre associé au sens pour constuire l'exemple + ajouter au dictionnaire w2sense\n",
    "        gold = int((re.findall(\"ws_[0-9]\",gold_line)[0]).replace(\"ws_\",\"\"))\n",
    "        \n",
    "        w2senses[instance].add(gold)\n",
    "        w2examples[instance].append((context,gold))\n",
    "        \n",
    "    return w2examples,w2senses\n",
    "\n",
    "w2examples,w2senses = extract_examples_and_senses(data_file,gold_file)\n",
    "instances = list(w2examples.keys())\n",
    "print(\"nombre d'instances présentes dans le corpus: \",len(instances),\"\\n\")\n",
    "print(\"instance et taille des données d'entraînement\",[(word,len(w2examples[word])) for word in w2examples],\"\\n\")\n",
    "\n",
    "#2) Facultatif : script pour créer le fichier texte ne comportant que les embeddings qui nous intéressent - plus rapide\n",
    "\n",
    "if creer_fichier_embeddings :\n",
    "    #Element à définir\n",
    "    fasstex_path = \"../fasstex\" \n",
    "\n",
    "    #création du vocabulaire du corpus entier\n",
    "    i2w = set()\n",
    "    for instance,examples in w2examples.items():\n",
    "        for context,gold in examples :\n",
    "            i2w.update(context)\n",
    "    i2w = list(i2w)\n",
    "\n",
    "    f = open(fasstex_path, \"r\", encoding=\"UTF-8\")\n",
    "    f.readline() #permet de ne pas prendre en compte la première ligne du fichier qui résumé ce que contient le fichier\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(embeddings_path,\"w\",encoding=\"UTF-8\") as f : \n",
    "            f.writelines([line for line in lines if line.split(\" \")[0] in i2w])\n",
    "\n",
    "#3) Construction du dictionnaire qui va nous permettre de faire le look up (matrice d'embeddings)\n",
    "\n",
    "def extract_embeddings(path_embeddings) :\n",
    "    \"\"\"Récupère les embeddings dans le fichier générée.\n",
    "\n",
    "    Args:\n",
    "        path_embeddings (string)\n",
    "\n",
    "    Returns:\n",
    "        dictionnary: Associe à chaque mot son embedding\n",
    "    \"\"\"\n",
    "    f = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "    #On récupère dans le fichier crée les embeddings pour créer un dictionnaire\n",
    "    w2emb = {}\n",
    "    for line in f.readlines():\n",
    "        splitted_line = line.split(\" \")\n",
    "        word = splitted_line[0]\n",
    "        embedding = list(map(float,splitted_line[1:]))\n",
    "        w2emb[word] = embedding\n",
    "    return w2emb\n",
    "\n",
    "w2emb = extract_embeddings(embeddings_path)\n",
    "#4) Sélection des instances pour la classification et opération de look up pour chacune d'elle\n",
    "\n",
    "#Elément à définir #EST-CE QUE LA ON MET LA FONCTION MAKE_BATCHES ?\n",
    "instances_to_test = instances[:3]\n",
    "\n",
    "\n",
    "def look_up(context, w2emb) :\n",
    "    \"\"\"Remplace dans le vecteur de contexte les mots par leur embedding.\n",
    "\n",
    "    Args:\n",
    "        context (list): liste de taille (size_window*2)+1\n",
    "        w2emb (dictionnary): Associe à chaque mot son embedding\n",
    "\n",
    "    Returns:\n",
    "        list : liste de taille size_embedding : BOW\n",
    "    \"\"\"\n",
    "    emb_size = len(list(w2emb.values())[0]) #on récupère la taille d'un embedding : 300\n",
    "    context_emb = np.zeros(emb_size)\n",
    "    for word in context :\n",
    "        if word in w2emb :\n",
    "            context_emb = np.add(context_emb, np.array(w2emb[word]))             \n",
    "    return context_emb\n",
    "\n",
    "#5) Sélection de données représentatives du corpus d'entraînement en fonction du nombre de données considérées\n",
    "size_data = [1,0.8,0.6,0.4,0.2]\n",
    "#Remarque : \"affecter\" compte 7 senses différents pour 25 examples et \"demeurer\",6 senses pour 21 examples.\n",
    "#Conséquence : on ne peut pas considérer moins de 25% des données annotées pour \"affecter\" et 30% pour \"demeurer\"\n",
    "\n",
    "def select_examples(examples,senses,size):\n",
    "    \"\"\"Choisit des examples d'entraînement représentatifs du corpus.\n",
    "\n",
    "    Args:\n",
    "        examples (list)\n",
    "        senses (int): nombre de senses associés à l'instance\n",
    "        size (float): quantité des données d'entraînement considérés\n",
    "\n",
    "    Returns:\n",
    "        list: examples qui contiennent au moins un example de chaque sense\n",
    "    \"\"\"\n",
    "    selected_examples = []\n",
    "    \n",
    "    #Pour chaque sens, on ajoute un example associé à ce sens ,au hasard\n",
    "    for sense in senses :\n",
    "        selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),examples))))\n",
    "    \n",
    "    #On calcule ensuite le nombre d'examples qu'il reste à ajouter pour atteindre la quantité de données souhaitée\n",
    "    size_to_add = round(size*(len(examples)))-len(selected_examples)\n",
    "    \n",
    "    #On ajoute ce nombre de données (non-présentes déjà dans la liste) selectionnées au hasard\n",
    "    selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),examples)),k=size_to_add))\n",
    "    \n",
    "    return selected_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotated_examples, not_annotated_examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "                    si example = ([1.9, 2.3, 0.6], 1),\n",
    "                    - le contexte avec le mot à désambiguiser et son lemme est \n",
    "                      l'embedding [1.9, 2.3, 0.6]\n",
    "                    - le numéro de sens est 1\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.annotated_examples = annotated_examples\n",
    "\n",
    "        # on a le gold, mais on ne l'utilisera que pour calculer l'accuracy\n",
    "        # pour le training, nous ferons comme si nous n'avions pas la gold class\n",
    "        self.not_annotated_examples = not_annotated_examples\n",
    "\n",
    "        # transforme les embeddings en tensors\n",
    "        # ce sont les exemples qui devront être classifiés\n",
    "        self.tensors_examples = [example[0] for example in self.not_annotated_examples]\n",
    "\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données annotées, qui représentent tous les sens possibles\n",
    "        self.k = len(set([example[1] for example in self.annotated_examples]))\n",
    "\n",
    "        \n",
    "        # initialisation des centroïdes : pour chaque sens, le centroïde \n",
    "        # correspond à la moyenne des embeddings des exemples annotés\n",
    "        # Ainsi, chaque centroïde représente un sens\n",
    "        self.tensors_centroids, self.cluster2sense = self.make_centroids()\n",
    "\n",
    "        #initialisation d'un dataframe à partir de self.tensors_centroids\n",
    "        # rows: cluster number (k), columns: dimension number (len(tensors_examples[0]))\n",
    "        self.new_centroids = pd.DataFrame(self.tensors_centroids).astype(\"float\")\n",
    "        #print(self.new_centroids.head())\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(not_annotated_examples))\n",
    "        #random number between 0 and k-1 for each example\n",
    "        #self.clusters = np.asarray([random.randint(0,self.k-1) for i in range(len(not_annotated_examples))])\n",
    "\n",
    "\n",
    "    def make_centroids(self):\n",
    "        cluster2sense = []\n",
    "        tensors_centroids = []\n",
    "        senses = set([example[1] for example in self.annotated_examples])\n",
    "        for sense in senses: \n",
    "            # on récupère les examples du sens\n",
    "            examples_sense = [example[0] for example in self.annotated_examples if example[1] == sense]\n",
    "            # on calcule le centroid du sens\n",
    "            centroid = torch.mean(torch.stack(examples_sense), dim=0)\n",
    "            # on ajoute le centroid à la liste des centroids\n",
    "            tensors_centroids.append(centroid)\n",
    "            # on ajoute le sens à la liste des sens\n",
    "            #L'index du sens correspond au numéro du cluster\n",
    "            cluster2sense.append(sense)\n",
    "\n",
    "        return tensors_centroids, cluster2sense\n",
    "    \n",
    "\n",
    "    def update_new_centroids_dataframe(self):\n",
    "        # étape 1: on regroupe tous les tenseurs des exemples ayant le même cluster dans self.clusters et on fait la moyenne\n",
    "        tmp_new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean() #shape : [k, len(tensors_examples[0])]\n",
    "        # étape 2: on détermine quels sont les clusters qui ont été assignés à au moins un exemple dans self.clusters\n",
    "        clus, counts = np.unique(self.clusters, return_counts=True)\n",
    "        #print(dict(zip(clus, counts)))\n",
    "        # étape 3: pour chaque cluster qui n'a pas d'exemple assigné, on insère un row vide dans tmp_new_centroids à l'index correspondant\n",
    "        row_to_insert = [[np.nan] * self.new_centroids.shape[1]]\n",
    "        for i in range(self.k): \n",
    "            if i not in clus:\n",
    "                tmp_new_centroids = pd.concat([tmp_new_centroids.iloc[:i], pd.DataFrame(row_to_insert, columns=tmp_new_centroids.columns), tmp_new_centroids.iloc[i:]]).reset_index(drop=True)\n",
    "        # étape 4: on remplace dans self.new_centroids les rows non vides de tmp_new_centroids \n",
    "        self.new_centroids.update(tmp_new_centroids)\n",
    "\n",
    "\n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        #while diff:\n",
    "        for i in range(10):\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "         \n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = (tensor_centroid - tensor_example).pow(2).sum(axis=0).sqrt()\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "\n",
    "            #mise à jour des centroïdes: \n",
    "            self.update_new_centroids_dataframe()\n",
    "            \n",
    "            tensors_new_centroids = []\n",
    "            \n",
    "            #pour chaque cluster (donc chaque ligne de new_centroids)\n",
    "            for i in range(len(self.new_centroids.index)):\n",
    "                centroid = []\n",
    "                #pour chaque dimension du vecteur moyen assigné au cluster\n",
    "                for j in range(len(self.new_centroids.columns)):\n",
    "                    # recréee le vecteur\n",
    "                    centroid.append(int(self.new_centroids.iat[i,j]))\n",
    "                #et rassemble les nouveaux tenseurs des centroides dans une liste\n",
    "                tensors_new_centroids.append(torch.tensor(centroid))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            \n",
    "            \n",
    "\n",
    "    def accuracy(self): \n",
    "        correct = 0\n",
    "        for i in range(len(self.not_annotated_examples)):\n",
    "            sens_attendu = self.not_annotated_examples[i][1]\n",
    "            sens_assigne = self.cluster2sense[int(self.clusters[i])]\n",
    "            if sens_attendu == sens_assigne:\n",
    "                correct += 1\n",
    "        return correct, len(self.not_annotated_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0% de données annotées\n",
      "--> Instance : aboutir\n",
      "Le sens le plus fréquent pour 'aboutir' est le sens 3 avec une proportion de 99.72 %\n",
      "Pour 'aboutir', l'accuracy est de 20.57 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_at_instance_time(instance, y, num_correct, num_examples):\n",
    "    print(\"--> Instance :\",instance)\n",
    "    #print(f\"Répartition des sens: {Counter(y)}\")\n",
    "    most_frequent_sense = max(y,key=y.count)\n",
    "    occurrence_of_most_frequent_sense = y.count(most_frequent_sense)/len(y)\n",
    "    print(f\"Le sens le plus fréquent pour '{instance}' est le sens {most_frequent_sense} avec une proportion de {round(occurrence_of_most_frequent_sense*100,2)} %\")\n",
    "    #print(f\"Proportion de données annotées considérées sur le corpus: {size*100} %\")\n",
    "    print(f\"Pour '{instance}', l'accuracy est de {round(num_correct/num_examples*100,2)} %\")\n",
    "    print()\n",
    "\n",
    "def display_macro_and_micro_averages(accuracy_instances, list_num_correct, list_num_examples):\n",
    "\n",
    "    accuracy_kmeans = sum(accuracy_instances)/len(accuracy_instances)\n",
    "    print(f\"Macro-average : {round(accuracy_kmeans,2)}\")\n",
    "    micro_average = sum(list_num_correct)/sum(list_num_examples)\n",
    "    print(f\"Micro-average : {round(micro_average,2)}\")\n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "def create_annotated_and_not_annotated_sets(): \n",
    "    annotated_examples = select_examples(w2examples[instance],w2senses[instance],size)\n",
    "\n",
    "    not_annotated_examples = [ex for ex in select_examples(w2examples[instance],w2senses[instance],100) if ex not in annotated_examples]\n",
    "        \n",
    "    X_annotated = [torch.from_numpy(look_up(context,w2emb)) for context,gold in annotated_examples]\n",
    "    y_annotated = [gold for context,gold in annotated_examples]\n",
    "\n",
    "    #regarder constante\n",
    "    X = [torch.from_numpy(look_up(context,w2emb)) for context,gold in not_annotated_examples]\n",
    "    y_gold = [gold for context,gold in not_annotated_examples]\n",
    "      \n",
    "\n",
    "    return X_annotated, y_annotated, X, y_gold\n",
    "        \n",
    "\n",
    "for size in size_data[1:] : #CHANGER SIZE DATA\n",
    "\n",
    "    accuracy_instances = []\n",
    "    list_num_correct = []\n",
    "    list_num_examples = []\n",
    "    \n",
    "    print(f'{size*100}% de données annotées')\n",
    "\n",
    "    for instance in instances_to_test :\n",
    "            \n",
    "  \n",
    "        X_annotated, y_annotated, X, y_gold = create_annotated_and_not_annotated_sets()\n",
    "\n",
    "        k_Means = K_Means([(X_annotated[i],y_annotated[i]) for i in range(len(X_annotated))], [(X[i],y_gold[i]) for i in range(len(X))])\n",
    "        k_Means.learn_clusters()\n",
    "        num_correct, num_examples = k_Means.accuracy()\n",
    "        \n",
    "        display_at_instance_time(instance, y_annotated + y_gold, num_correct, num_examples)\n",
    "\n",
    "        list_num_correct.append(num_correct)\n",
    "        list_num_examples.append(num_examples)\n",
    "        accuracy_instances.append(num_correct/num_examples)\n",
    "\n",
    "    display_macro_and_micro_averages(accuracy_instances, list_num_correct, list_num_examples)    \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
