{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# WSD\n",
    "\n",
    "Ce notebook permet d'effectuer une comparaison des performances de deux algorithmes, un d'apprentissage supervisé et un d'apprentissage semi-supervisé, pour la tâche de Word Sense Disambiguation (WSD). \n",
    "\n",
    "Les deux méthodes sont les suivants :\n",
    "- pour la classification supervisée, utilisation d'un MLP\n",
    "- pour la classification semi-supervisée, utilisation d'un algorithme de constrained K-Means\n",
    "\n",
    "Plusieurs tests sont effectués en considérant plusieurs mots à désambiguiser pour lesquels les performances de ces deux méthodes sont évaluées.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extraction des données\n",
    "\n",
    "La classe Extractor permet d'accéder aux données et de les stocker dans des dictionnaires puis des embeddings. Elle permet de figer les données sur lesquelles les tests de classification seront ensuite faits. Elle comporte les méthodes suivantes :\n",
    "\n",
    "- `extract_examples()` : pour extraire les données d'entraînement et de test provenant de fichiers XML FSE\n",
    "\n",
    "- `extract_embeddings()` : pour extraire les embeddings à partir d'un fichier créé en amont, qui ne regroupe que les embeddings nécéssaires à notre jeu de données. Cette méthode est surtout utile pour drastiquement réduire le temps de chargement des embeddings\n",
    "\n",
    "- `look_up()` : pour effectuer l'étape de look-up avant la classifcation\n",
    "\n",
    "- `select_examples()` : pour sélectionner des données représentatives lorsqu'on ne considère pas toutes les données annotées. Pour qu'un ensemble de données soit représentatif, il faut que chaque étiquette présente dans les données totales soit présente au moins une fois dans le set d'entraînement sélectionné\n",
    "\n",
    "- `define_instance()` : pour choisir les données qui seront classifiées, c'est-à-dire quelle proportion des données seront utilisées (80%, 50%, 25%...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor :\n",
    "    \n",
    "    def __init__(self, data_path, gold_path, embeddings_path, context_size):\n",
    "        '''\n",
    "        A partir des données, extrait les examples (w2examples, w2senses) qui seront utilisés \n",
    "        lors de l'apprentissage.\n",
    "\n",
    "        INPUT\n",
    "        data_path (str) : chemin pour accéder aux données d'entraînement, au format XML\n",
    "        gold_path (str) : chemin pour accéder aux numéros de sens correspondant aux données\n",
    "                          d'entraînement\n",
    "        embeddings_path (str) : chemin pour accéder aux embeddings, déjà extraits et \n",
    "                                présélectionnés pour ces données\n",
    "        context_size (int) : taille de la fenêtre du contexte du mot à désambiguiser\n",
    "\n",
    "        ARGUMENTS DE CLASSE\n",
    "        self.w2examples (dict) : associe chaque lemme à une liste d'examples correspondant à son \n",
    "                                 utilisation dans une phrase du corpus. \n",
    "                                 examples (list) : liste de tous les couples \n",
    "                                  ([mots du contexte], numéro de sens) pour un lemme donné\n",
    "        self.w2senses (dict) : associe chaque lemme à son ensemble de sens possible\n",
    "        self.w2emb (dict) : associe chaque mot à son embedding\n",
    "        self.selected_examples (liste) : liste d'examples qui contient au moins un example de \n",
    "                                         chaque sens du lemme choisi\n",
    "        self.selected_examples_embs (liste) : même liste d'examples, mais avec les contextes \n",
    "                                              sous forme d'embedding\n",
    "        self.X_train (set) : corpus d'apprentissage\n",
    "        self.X_test (set) : numéros des sens correspondant au corpus d'apprentissage\n",
    "        self.y_train (set) : corpus de test\n",
    "        self.y_test (set) : numéros des sens correspondant au corpus d'apprentissage\n",
    "        '''\n",
    "        \n",
    "        #récupération des données XML d'apprentissage fournies en argument\n",
    "        tree = ET.parse(data_path)\n",
    "        data_file = tree.getroot()\n",
    "\n",
    "        #récupération des données txt correspondant aux gold class\n",
    "        gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "        \n",
    "        # self.w2examples (dict) : associe chaque lemme à une liste d'examples correspondant à son \n",
    "        #                          utilisation dans une phrase du corpus.\n",
    "        # clé (str) : lemme du mot à désambiguiser\n",
    "        # valeur (list) : liste de tous les couples ([mots du contexte], numéro de sens) pour ce \n",
    "        #                 lemme\n",
    "        #\n",
    "        # self.w2senses (dict) : associe chaque lemme à son ensemble de sens possible\n",
    "        # clé (str) : lemme du mot à désambiguiser\n",
    "        # valeur (set) : ensemble des sens possible pour ce lemme\n",
    "        self.w2examples, self.w2senses = self.extract_examples_and_senses(data_file, gold_file, context_size)\n",
    "\n",
    "        # self.w2emb (dict) : associe chaque mot à l'embedding représentant son contexte sommé\n",
    "        # clé (str) : mot à désambiguiser\n",
    "        # valeur (list) : embedding\n",
    "        self.w2emb = self.extract_embeddings(embeddings_path)\n",
    "\n",
    "        # liste d'examples qui contient au moins un example de chaque sens du lemme choisi\n",
    "        # cette liste sera définie lorsque l'instance du mot à désambiguiser sera choisi\n",
    "        self.selected_examples = []\n",
    "        # même liste d'examples, mais avec les contextes sous forme d'embedding\n",
    "        # cette liste sera définie lorsque l'instance du mot à désambiguiser sera choisi\n",
    "        self.selected_examples_embs = []\n",
    "\n",
    "        # les corpus seront définis lorsque l'instance du mot à désambiguiser sera choisi\n",
    "        self.X_train = set()\n",
    "        self.X_test = set()\n",
    "        self.y_train = set()\n",
    "        self.y_test = set()\n",
    "    \n",
    "    def extract_examples_and_senses(self, data_file, gold_file, context_size):\n",
    "        '''\n",
    "        Extrait les données à partir des fichiers de corpus d'apprentissage et de gold classes.\n",
    "\n",
    "        INPUT\n",
    "        data_file (Element): représentation des phrases du corpus d'apprentissage\n",
    "        gold_file (TextIOWrapper): fichier contenant les numéros de sens (gold class) pour chaque\n",
    "                                   mot à désambiguiser\n",
    "        '''\n",
    "    \n",
    "        # clé (str) : lemme du mot à désambiguiser\n",
    "        # valeur (list) : liste de tous les couples ([mots du contexte], numéro de sens) pour ce \n",
    "        #                 lemme\n",
    "        w2examples = {}\n",
    "        # clé (str) : lemme du mot à désambiguiser\n",
    "        # valeur (set) : ensemble des sens possible pour ce lemme\n",
    "        w2senses = defaultdict(set)\n",
    "\n",
    "        # lecture du fichier gold\n",
    "        gold_file = gold_file.readlines()\n",
    "        \n",
    "        # index de parcours dans le fichier gold\n",
    "        i_gold = 0\n",
    "\n",
    "        # pour chaque phrase du corpus\n",
    "        for text_id in data_file:\n",
    "            for sentence in text_id:\n",
    "\n",
    "                # recherche de(s) l'indice(s) de(s) l'instance(s) pour savoir où se trouve la fenêtre\n",
    "                i_instances = []\n",
    "                for j in range(len(sentence)):\n",
    "                    if sentence[j].tag == \"instance\":\n",
    "                        i_instances.append(j)\n",
    "                \n",
    "                # tant qu'il y a des instances à repérer dans la phrase\n",
    "                while len(i_instances) > 0:\n",
    "\n",
    "                    # enregistrement du lemme de l'instance\n",
    "                    instance = sentence[i_instances[0]].attrib[\"lemma\"].lower()\n",
    "                    \n",
    "                    # si l'instance n'a pas encore de contexte\n",
    "                    if instance not in w2examples : \n",
    "                        # le crée\n",
    "                        w2examples[instance] = []\n",
    "\n",
    "                    context = []\n",
    "                    # pour chaque mot de la fenêtre\n",
    "                    for k in range(-context_size, context_size + 1):\n",
    "                        # si le mot est l'instance, enregistrement de son lemme\n",
    "                        if k == 0:\n",
    "                            context.append(instance)\n",
    "                        # sinon enregistrement du mot du contexte\n",
    "                        elif len(sentence) > i_instances[0] + k and i_instances[0] + k >= 0:\n",
    "                            context.append(sentence[i_instances[0] + k].text.lower())\n",
    "                    \n",
    "                    # récupération des différents sens possibles dans le fichier gold \n",
    "                    gold_class = gold_file[i_gold].split(\"__\")[1].split(\"_\")[1]\n",
    "\n",
    "                    # le fichiers gold et data ayant les données dans le même ordre, les instances\n",
    "                    # et les sens peuvent être enregistrés en même temps dans leur dictionnaire\n",
    "                    # respectif\n",
    "                    w2senses[instance].add(gold_class)\n",
    "                    w2examples[instance].append((context, gold_class))\n",
    "                    \n",
    "                    # l'instance lue est enlevée de la to-do list et l'index de parcours du fichier \n",
    "                    # gold est incrémenté pour passer à l'instance suivante\n",
    "                    i_instances.pop(0)\n",
    "                    i_gold += 1\n",
    "\n",
    "        return w2examples, w2senses\n",
    "    \n",
    "    def extract_embeddings(self,path_embeddings) :\n",
    "        '''\n",
    "        Récupère les embeddings dans le fichier généré et associe à chaque mot son\n",
    "        embedding.\n",
    "\n",
    "        INPUT\n",
    "        path_embeddings (str) : chemin pour accéder aux embeddings, déjà extraits et \n",
    "                                présélectionnés pour ces données\n",
    "\n",
    "        OUTPUT\n",
    "        w2emb (dict) : associe chaque mot à son embedding\n",
    "        '''\n",
    "\n",
    "        # lecture du fichier\n",
    "        file_embeddings = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "        # clé (str) : mot à désambiguiser\n",
    "        # valeur (list) : embedding du mot\n",
    "        w2emb = {}\n",
    "\n",
    "        # pour chaque mot du fichier\n",
    "        for line in file_embeddings.readlines():\n",
    "\n",
    "            # séparation du mot et de l'embedding\n",
    "            splitted_line = line.split(\" \")\n",
    "            word = splitted_line[0]\n",
    "            embedding = list(map(float,splitted_line[1:]))\n",
    "            # insertion du mot et de son embedding dans le dictionnaire\n",
    "            w2emb[word] = embedding\n",
    "\n",
    "        return w2emb\n",
    "\n",
    "    def look_up(self, context) :\n",
    "        '''\n",
    "        Remplace dans les mots du vecteur de contexte par leur embedding et en fait la\n",
    "        somme.\n",
    "\n",
    "        INPUT\n",
    "        context (list) : liste de taille (size_window*2)+1\n",
    "\n",
    "        OUTPUT\n",
    "        context_emb (list) : liste de taille size_embedding (300 ici)\n",
    "        '''\n",
    "\n",
    "        # taille d'un embedding : 300\n",
    "        emb_size = len(list(self.w2emb.values())[0]) \n",
    "        # initialisation du contexte sous forme d'embedding\n",
    "        context_emb = np.zeros(emb_size)\n",
    "\n",
    "        # pour chaque mot du contexte\n",
    "        for word in context :\n",
    "            # s'il est présent dans le dictionnaire d'embeddings\n",
    "            if word in self.w2emb :\n",
    "                # il est ajouté à l'embedding représentant le contexte\n",
    "                context_emb = np.add(context_emb, np.array(self.w2emb[word])) \n",
    "\n",
    "        return context_emb\n",
    "    \n",
    "    def select_examples(self, examples, senses, sample_size):\n",
    "        '''\n",
    "        Choisit des examples d'entraînement représentatifs du corpus selon un nombre\n",
    "        imposé de données à choisir.\n",
    "\n",
    "        INPUT\n",
    "        examples (list) : liste d'examples pour le mot à désambiguiser\n",
    "        n_senses (int) : nombre de senses associés à ce mot\n",
    "        sample_size (float) : quantité des données d'entraînement considérés\n",
    "\n",
    "        OUTPUT\n",
    "        selected_examples (list) : liste d'examples qui contient au moins un example \n",
    "                                   de chaque sens\n",
    "        '''\n",
    "\n",
    "        selected_examples = []\n",
    "        \n",
    "        # pour chaque sens, on ajoute un example associé à ce sens, au hasard\n",
    "        for sense in senses :\n",
    "            selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),\n",
    "                                                               examples))))\n",
    "        print(len(senses), len(selected_examples))#TODO\n",
    "        # calcul du nombre d'examples qu'il reste à ajouter pour atteindre la quantité \n",
    "        # de données souhaitée\n",
    "        size_to_add = round(sample_size*(len(examples)))-len(selected_examples)\n",
    "        \n",
    "        # ajoute ce nombre de données (non-présentes dans la liste) selectionnées au hasard\n",
    "        selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),\n",
    "                                                            examples)),\n",
    "                                                    k=size_to_add))\n",
    "\n",
    "        return selected_examples\n",
    "    \n",
    "    def select_train(x,y,senses,train_size=0.8):\n",
    "\n",
    "            #pour chaque sens, on choisit au hasard un context et son étiquette\n",
    "            selected_train = [random.choice(list(filter((lambda example:example[1]==sense),\n",
    "                                                                zip(x,y)))) for sense in senses]\n",
    "            \n",
    "            #on choisit ensuite au hasard un même nombre de contextes et d'étiquettes différentes\n",
    "            selected_test =random.choices(list(filter((lambda example : example not in selected_train),\n",
    "                                                                zip(x,y))), k=len(selected_train))\n",
    "            \n",
    "            #on prend le reste d'example pour effectuer le split \n",
    "            remaining_X, remaining_y = zip(*(filter((lambda example:example not in selected_train and example not in selected_test),\n",
    "                                                                zip(x,y))))\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(remaining_X, remaining_y, train_size)\n",
    "            \n",
    "            #une fois le split effectué, on ajoute les données selectionnées en amont\n",
    "            selected_X_train, selected_y_train = zip(*selected_train)\n",
    "            selected_X_test, selected_y_test = zip(*selected_test)\n",
    "            \n",
    "            X_train.extend(selected_X_train)\n",
    "            X_test.extend(selected_X_test)\n",
    "            y_train.extend(selected_y_train)\n",
    "            y_test.extend(selected_y_test)\n",
    "            \n",
    "            return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def define_instance(self, instance, data_size, affichage=True):\n",
    "        '''\n",
    "        Permet de définir l'instance du mot à désambiguiser et les examples à utiliser. \n",
    "        Fixe les derniers arguments de classe selon l'instance choisie.\n",
    "        Contraint le choix de classification selon le nombre de données considéré.\n",
    "\n",
    "        INPUT\n",
    "        instance (str) : mot à désambiguiser\n",
    "        data_size (float): quantité de données à considérer\n",
    "        affichage (bool, optional): affiche une trace des options choisies\n",
    "        '''\n",
    "\n",
    "        # choisit des examples d'entraînement représentatifs du corpus selon le nombre\n",
    "        # imposé de données\n",
    "        self.selected_examples = self.select_examples(self.w2examples[instance],\n",
    "                                                      self.w2senses[instance],\n",
    "                                                      data_size)\n",
    "\n",
    "        # liste de tous les contextes sous forme d'embedding\n",
    "        x = [self.look_up(context) for context, gold in self.selected_examples]\n",
    "        # liste de tous les numéros de sens\n",
    "        y = [gold for context, gold in self.selected_examples]\n",
    "        \n",
    "        # liste des examples d'entrainement représentatifs sous forme d'embeddings\n",
    "        self.selected_examples_embs = list(zip(x,y))\n",
    "        \n",
    "        # création des corpus d'entrainement et de test\n",
    "        #self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, train_size=0.8)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.select_train(x, y, self.w2senses[instance])\n",
    "        \n",
    "        # si une trace est souhaitée\n",
    "        if affichage :\n",
    "            print(\"instance :\", instance)\n",
    "            print(f'{data_size*100}% des données annotées considérées')\n",
    "            print(\"nombre de données d'entraînement : \", len(self.X_train))\n",
    "            print(\"étiquettes possibles pour cette instance : \", self.w2senses[instance])\n",
    "            print(\"étiquettes présentes dans les données d'entraînement :\", Counter(self.y_train)) # TODO : ce n'est pas représentatif car il manque souvent au moins une étiquette\n",
    "        return\n",
    "    \n",
    "    def get_sets(self):\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def get_examples(self):\n",
    "        return self.selected_examples\n",
    "    \n",
    "    def get_embs(self):\n",
    "        return self.selected_examples_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin du corpus annoté\n",
    "data_path = \"../donnees/FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "# chemin pour récupérer les gold class du corpus annoté\n",
    "gold_path = \"../donnees/FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "# choix de la fenêtre du contexte\n",
    "context_size = 4\n",
    "# chemin pour récupérer les embeddings afin de faire l'opération look-up\n",
    "# Les embeddings sont extraits de fasttext\n",
    "embeddings_path = \"embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Extractor' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ext \u001b[39m=\u001b[39m Extractor(data_path, gold_path, embeddings_path, context_size)\n\u001b[1;32m----> 2\u001b[0m ext\u001b[39m.\u001b[39;49mdefine_instance(\u001b[39m\"\u001b[39;49m\u001b[39maboutir\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m ext\u001b[39m.\u001b[39mget_sets()\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mexemple d\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexample avec contexte et gold class :\u001b[39m\u001b[39m\"\u001b[39m,ext\u001b[39m.\u001b[39mget_examples()[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[68], line 286\u001b[0m, in \u001b[0;36mExtractor.define_instance\u001b[1;34m(self, instance, data_size, affichage)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_examples_embs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(x,y))\n\u001b[0;32m    284\u001b[0m \u001b[39m# création des corpus d'entrainement et de test\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m#self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, train_size=0.8)\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m sets \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_train(x, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw2senses[instance])\n\u001b[0;32m    287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_test \u001b[39m=\u001b[39m sets\n\u001b[0;32m    289\u001b[0m \u001b[39m# si une trace est souhaitée\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[68], line 234\u001b[0m, in \u001b[0;36mExtractor.select_train\u001b[1;34m(x, y, senses, train_size)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_train\u001b[39m(x,y,senses,train_size\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m):\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m         \u001b[39m#pour chaque sens, on choisit au hasard un context et son étiquette\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m         selected_train \u001b[39m=\u001b[39m [random\u001b[39m.\u001b[39mchoice(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m((\u001b[39mlambda\u001b[39;00m example:example[\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39msense),\n\u001b[0;32m    235\u001b[0m                                                             \u001b[39mzip\u001b[39m(x,y)))) \u001b[39mfor\u001b[39;00m sense \u001b[39min\u001b[39;00m senses]\n\u001b[0;32m    237\u001b[0m         \u001b[39m#on choisit ensuite au hasard un même nombre de contextes et d'étiquettes différentes\u001b[39;00m\n\u001b[0;32m    238\u001b[0m         selected_test \u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mchoices(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m((\u001b[39mlambda\u001b[39;00m example : example \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m selected_train),\n\u001b[0;32m    239\u001b[0m                                                             \u001b[39mzip\u001b[39m(x,y))), k\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(selected_train))\n",
      "Cell \u001b[1;32mIn[68], line 235\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_train\u001b[39m(x,y,senses,train_size\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m):\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m         \u001b[39m#pour chaque sens, on choisit au hasard un context et son étiquette\u001b[39;00m\n\u001b[0;32m    234\u001b[0m         selected_train \u001b[39m=\u001b[39m [random\u001b[39m.\u001b[39mchoice(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m((\u001b[39mlambda\u001b[39;00m example:example[\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39msense),\n\u001b[1;32m--> 235\u001b[0m                                                             \u001b[39mzip\u001b[39;49m(x,y)))) \u001b[39mfor\u001b[39;00m sense \u001b[39min\u001b[39;00m senses]\n\u001b[0;32m    237\u001b[0m         \u001b[39m#on choisit ensuite au hasard un même nombre de contextes et d'étiquettes différentes\u001b[39;00m\n\u001b[0;32m    238\u001b[0m         selected_test \u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mchoices(\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m((\u001b[39mlambda\u001b[39;00m example : example \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m selected_train),\n\u001b[0;32m    239\u001b[0m                                                             \u001b[39mzip\u001b[39m(x,y))), k\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(selected_train))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Extractor' object is not iterable"
     ]
    }
   ],
   "source": [
    "ext = Extractor(data_path, gold_path, embeddings_path, context_size)\n",
    "ext.define_instance(\"aboutir\",1)\n",
    "X_train, X_test, y_train, y_test = ext.get_sets()\n",
    "print(\"\\nexemple d'example avec contexte et gold class :\",ext.get_examples()[0])\n",
    "print(\"\\nau format embedding : \",ext.get_embs()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification supervisé\n",
    "\n",
    "On construit une classe Classifieur qui va nous permettre de prédire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifieur :\n",
    "    \n",
    "    def __init__(self,X_train, X_test, y_train, y_test) :\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test \n",
    "        \n",
    "    def classify(self,affichage=False) :\n",
    "        \"\"\"Permet de prédire et d'afficher les données de classification.\n",
    "\n",
    "        Args:\n",
    "            affichage (bool, optional): Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list: prédiction\n",
    "        \"\"\"\n",
    "        clf = MLPClassifier(random_state=1,hidden_layer_sizes=(100,)) \n",
    "        clf.fit(self.X_train, self.y_train)\n",
    "        y_pred = clf.predict(self.X_test)\n",
    "        \n",
    "        if affichage :\n",
    "            print(\"prediction :\",y_pred)\n",
    "            print(\"gold :\",self.y_test)\n",
    "            print(\"accuracy score : \", accuracy_score(y_pred,self.y_test),\"\\n\")\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : ['4' '3' '3' '3' '3' '4' '3' '3' '3' '4']\n",
      "gold : ['4', '3', '3', '3', '3', '4', '3', '3', '3', '4']\n",
      "accuracy score :  1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['4', '3', '3', '3', '3', '4', '3', '3', '3', '4'], dtype='<U1')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clf = Classifieur(X_train,X_test,y_train,y_test)\n",
    "Clf.classify(affichage=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un second temps, on effectue nos tests sur plusieurs classifieurs en choisissant un pas de descente dans la quantité des données annotées considérées. Pour chaque classifieur et chaque quantitée de données considérées, on effectue n_repeat classifications pour obtenir une accuracy moyenne représentative du classifieur. Par conséquent, pour n_repeat=5 et step=0.25, nous obtiendrons pour chaque classifieur une liste d'accuracies correspondante à la moyenne des accuracies de 5 prédictions pour 100%, 75%, 50% et 25% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_accuracy(instance,data_size,n_repeat,affichage=True):\n",
    "        \"\"\"Permet de retourner une moyenne d'accuracies pour un nombre de classifications donné.\n",
    "\n",
    "        Args:\n",
    "            n_repeat (int): nombre de classifications à tester\n",
    "            affichage (bool, optional): Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            int: moyenne des accuracies computed\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        \n",
    "        for i in range(n_repeat) :\n",
    "            \n",
    "            #pour chaque test, on crée un nouveau classifieur, qui s'appuie toujours sur la même quantité de données mais avec un autre tirage\n",
    "            #par exemple, pour 75% des données, on obtiendra des accuracies différentes en fonction des tirages de ces données\n",
    "            ext.define_instance(instance,data_size,False)\n",
    "            X_train, X_test, y_train, y_test = ext.get_sets()\n",
    "            Clf = Classifieur(X_train, X_test, y_train, y_test)\n",
    "            y_pred = Clf.classify()\n",
    "            \n",
    "            accuracies.append(accuracy_score(y_pred,y_test))\n",
    "            mean = sum(accuracies)/len(accuracies)\n",
    "            \n",
    "        if affichage :\n",
    "            print(f'{data_size*100}% des données annotées considérées')\n",
    "            print(\"instance :\",instance)\n",
    "            print(\"accuracies :\",accuracies)\n",
    "            print(\"accuracy moyenne :\",mean)\n",
    "            print()\n",
    "        return mean\n",
    "\n",
    "def get_accuracies(instances,step,n_repeat,affichage=False):\n",
    "            \"\"\"Permet d'obtenir une accuracy moyenne par lemme pour une certaine quantité de données considérée.\n",
    "            Produit un fichier csv.\n",
    "\n",
    "            Args:\n",
    "                instances (string): mot à désambiguiser\n",
    "                step (float): pas de descente dans la quantité de données à considérer\n",
    "                n_repeat (int): nombre de classifications tests à effectuer\n",
    "\n",
    "            Returns:\n",
    "                dictionnary: associe à chaque instance sa liste d'accuracies moyenne \n",
    "            \"\"\"\n",
    "            instance2acc = {instance : [] for instance in instances}\n",
    "            moyennes = []\n",
    "            data_sizes =[]\n",
    "            \n",
    "            for i in range(round(1.0/step)): #pour chaque quantité de données observée\n",
    "                \n",
    "                data_size = 1.0 - (step*float(i))\n",
    "                data_sizes.append(data_size)\n",
    "                \n",
    "                for instance in instances : #pour chaque mot ambigü sélectionné\n",
    "                    instance2acc[instance].append(get_mean_accuracy(instance,data_size,n_repeat,affichage)) #on récupère son accuracy\n",
    "\n",
    "                #on boucle sur les valeurs obtenues pour obtenir leur moyenne\n",
    "                print(list(instance2acc.values()))\n",
    "                print([accuracies[i] for accuracies in instance2acc.values()])\n",
    "                moyennes.append(sum([accuracies[i] for accuracies in instance2acc.values()])/len(instance2acc))\n",
    "  \n",
    "            instance2acc[\"moyennes\"] = moyennes\n",
    "            \n",
    "            tab = instance2acc.copy()\n",
    "            tab[\"data_sizes\"] = data_sizes \n",
    "            df = pd.DataFrame(tab)\n",
    "            df.set_index(\"data_sizes\")\n",
    "            df.to_csv(f\"accuracies_par_lemme_{n_repeat}_repet.csv\")\n",
    "            return instance2acc     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A définir\n",
    "#Nombre de classifieurs choisis au hasard à tester\n",
    "#n_rand_instances = 3\n",
    "#Pas de descente dans la quantité de données considérées\n",
    "step = 0.25\n",
    "#Nombre de classifications pour un classifieur pour obtenir une accuracy moyenne\n",
    "n_repeat = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "6 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "6 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "6 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "8 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "8 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "4 4\n",
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "2 2\n",
      "2 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "#instances = random.choices(list(ext.w2examples.keys()),k=n_rand_instances)\n",
    "instances = list(ext.w2examples.keys())\n",
    "get_accuracies(instances,step,n_repeat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification semi-supervisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.examples = list(examples)\n",
    "        # transforme les embeddings en tensors\n",
    "        self.tensors_examples = [example[0] for example in self.examples]\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données\n",
    "        self.k = self.nb_senses()\n",
    "        # initialisation de centroids : pour chaque sens, un example est pris au hasard\n",
    "        self.tensors_centroids = [random.choice(example) \n",
    "                                  for example in self.examples_of_same_sense().values()]\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(examples))\n",
    "\n",
    "    def nb_senses(self):\n",
    "        '''\n",
    "        Renvoie le nombre de sens existants dans un ensemble d'examples\n",
    "        '''\n",
    "\n",
    "        known_senses = []\n",
    "        # pour chaque exemple\n",
    "        for example in self.examples:\n",
    "            # si le sens attendu (gold class) n'a pas encore été rencontré\n",
    "            if example[1] not in known_senses:\n",
    "                # l'ajoute à la liste des sens possibles\n",
    "                known_senses.append(example[1])\n",
    "        # renvoie le nombre de sens\n",
    "        return len(known_senses)\n",
    "    \n",
    "    def examples_of_same_sense(self):\n",
    "        '''\n",
    "        Regroupe les contextes des examples dans un dictionnaire triés selon le \n",
    "        sens du mot à désambiguiser\n",
    "        '''\n",
    "\n",
    "        # clé : numéro du sens\n",
    "        # valeur : liste de contextes avec ce sens en gold class\n",
    "        sense2examples = {}\n",
    "        # pour chaque example\n",
    "        for example in self.examples:\n",
    "            # si sa gold class n'a pas été déjà rencontrée\n",
    "            if example[1] not in sense2examples:\n",
    "                # ajoute une clé pour cette gold class\n",
    "                sense2examples[example[1]] = []\n",
    "            # ajoute le contexte au dictionnaire correspondant au sens utilisé\n",
    "            sense2examples[example[1]].append(example[0])\n",
    "\n",
    "        return sense2examples\n",
    "    \n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        while diff:\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "\n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = 0\n",
    "                    for k in range(len(tensor_example)):\n",
    "                        d += (tensor_centroid[k].item() - tensor_example[k].item())**2\n",
    "                    d = np.sqrt(d)\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "            # CALCUL DES NOUVEAUX CENTROIDES\n",
    "\n",
    "            # calcul des nouveaux centroides en utilisant le point au milieu de tous les\n",
    "            # autres points du même cluster\n",
    "            new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean()\n",
    "            # transforme ces nouveaux centroides en tensors\n",
    "            tensors_new_centroids = []\n",
    "            for i in range(len(new_centroids.index)):\n",
    "                colums = []\n",
    "                for j in range(len(new_centroids.columns)):\n",
    "                    colums.append(int(new_centroids.iat[i,j]))\n",
    "                tensors_new_centroids.append(torch.tensor(colums))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "equal(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m examples \u001b[39m=\u001b[39m ext\u001b[39m.\u001b[39mget_embs()\n\u001b[1;32m      2\u001b[0m k_means \u001b[39m=\u001b[39m K_Means(examples)\n\u001b[0;32m----> 3\u001b[0m k_means\u001b[39m.\u001b[39;49mlearn_clusters()\n",
      "Cell \u001b[0;32mIn[81], line 118\u001b[0m, in \u001b[0;36mK_Means.learn_clusters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m# pour chaque centroide\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors_centroids)):\n\u001b[1;32m    117\u001b[0m     \u001b[39m# si l'ancien centroide et le nouveau ne sont pas les mêmes\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m(torch\u001b[39m.\u001b[39;49mequal(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors_centroids[i], tensors_new_centroids[i])):\n\u001b[1;32m    119\u001b[0m         count_diff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[39m# met à jour le centroide\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: equal(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "examples = ext.get_embs()\n",
    "k_means = K_Means(examples)\n",
    "k_means.learn_clusters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
