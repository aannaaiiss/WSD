{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook permet d'effectuer des tests de comparaison des performances d'apprentissage supervisée et semi-supervisée pour la tâche de Word Sense Disambiguation. Nous développerons deux méthodes de classification : un MLP pour la classification supervisée et un constrained K-means pour la classification semi-supervisée. Nous effectuerons plusieurs tests en considérant plusieurs mots à désambiguiser pour lesquels nous évaluerons les performances de ces deux méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe Extractor permet d'accéder aux données et de les stocker dans un format traitable. Elle permet de figer les données sur lesquelles les tests de classification seront faites ensuite. Elle comporte différentes méthodes:  \n",
    "        1 : extract_examples() : pour extraire les données d'entraînement et de test  \n",
    "        2 : extract_embeddings() : pour extraire les embeddings à partir d'un fichier crée en amont, qui ne regroupe que les embeddings nécéssaires à notre jeu de données  \n",
    "        3 : look_up() : pour effectuer l'étape de look-up avant la classifcation en elle-même  \n",
    "        4 : select_examples() :  pour sélectionner des données représentatives lorsqu'on ne considère pas toutes les données annotées i.e. chaque étiquette présente dans les données est présente au moins une fois dans le set d'entraînement  \n",
    "        5 : define_instance() : permet de choisir les données qui seront classifiées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor :\n",
    "    \n",
    "    def __init__(self,data_path,gold_path,embeddings_path,context_size):\n",
    "        \n",
    "        #récupération des données XML\n",
    "        tree = ET.parse(data_path)\n",
    "        data_file = tree.getroot()[0]\n",
    "\n",
    "        #récupération des données .txt\n",
    "        gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "        \n",
    "        self.w2examples, self.w2senses = self.extract_examples_and_senses(data_file,gold_file,context_size)\n",
    "        self.w2emb = self.extract_embeddings(embeddings_path)\n",
    "    \n",
    "    def extract_examples_and_senses(self,data_file, gold_file, context_size):\n",
    "        \"\"\"Extract the data from the files.\n",
    "\n",
    "        Args:\n",
    "            data_file (Element): Sentences\n",
    "            gold_file (TextIOWrapper): Golds keys\n",
    "\n",
    "        Returns:\n",
    "            dictionary: associates the list of context vectors corresponding to the instance\n",
    "            dictionary : associates to the word each senses\n",
    "        \"\"\"\n",
    "    \n",
    "        w2examples={}\n",
    "        w2senses = defaultdict(set)\n",
    "        \n",
    "        for (sentence,gold_line) in zip(data_file,gold_file.readlines()) :\n",
    "            \n",
    "            #pour chaque phrase, on initialise deux listes qui permettront de respecter les tailles des contextes (+10,-10)\n",
    "            context_before = []\n",
    "            context_after = []\n",
    "            context = []\n",
    "            \n",
    "            #on boucle sur les mots de la phrase pour construire les listes\n",
    "            #on cherche l'instance et on repart en arrière pour constuire le contexte avant\n",
    "            i_instance = 0\n",
    "            while sentence[i_instance].tag != \"instance\" : \n",
    "                i_instance+=1\n",
    "            \n",
    "            instance = sentence[i_instance].attrib[\"lemma\"].lower()\n",
    "            \n",
    "            if instance not in w2examples : \n",
    "                w2examples[instance] = []\n",
    "            \n",
    "            #on vérifie la longueur des phrases pour ne pas soulever d'erreur\n",
    "            \n",
    "            #context_before \n",
    "            \n",
    "            #si le contexte avant l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance-1 to i_instance-5\n",
    "            if (len(sentence[:i_instance])>=context_size) :\n",
    "                    for i in range(1,context_size+1) :\n",
    "                        context_before.append(sentence[i_instance-i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(1,len(sentence[:i_instance])+1) :\n",
    "                    context_before.append(sentence[i_instance-i].text.lower())\n",
    "\n",
    "            #context_after\n",
    "            \n",
    "            #si le contexte après l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance+1 to i_instance+11\n",
    "            if(len(sentence[i_instance+1:])>= context_size) :\n",
    "                for i in range(i_instance+1,i_instance+(context_size+1)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(i_instance+1,len(sentence)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #une fois les listes constituées, on ajoute les balises de début et de fin de phrase si nécessaire\n",
    "            for i in range(context_size-len(context_before)) :\n",
    "                context_before.append(\"<d>\")\n",
    "                \n",
    "            for i in range(context_size-len(context_after)) :\n",
    "                context_after.append(\"<f>\")\n",
    "                \n",
    "            #le vecteur sera une concaténation des contextes d'avant et d'après\n",
    "            context = context_before\n",
    "            context.append(instance)\n",
    "            context.extend(context_after)\n",
    "                \n",
    "            #on récupère ensuite le nombre associé au sens pour constuire l'exemple + ajouter au dictionnaire w2sense\n",
    "            gold = int((re.findall(\"ws_[0-9]\",gold_line)[0]).replace(\"ws_\",\"\"))\n",
    "            \n",
    "            w2senses[instance].add(int(gold))\n",
    "            w2examples[instance].append((context,int(gold)))\n",
    "            \n",
    "        return w2examples,w2senses\n",
    "    \n",
    "    def extract_embeddings(self,path_embeddings) :\n",
    "        '''\n",
    "        Récupère les embeddings dans le fichier générée.\n",
    "\n",
    "        Args:\n",
    "            path_embeddings (string)\n",
    "\n",
    "        Returns:\n",
    "            dictionnary: Associe à chaque mot son embedding\n",
    "        '''\n",
    "\n",
    "        f = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "        #On récupère dans le fichier crée les embeddings pour créer un dictionnaire\n",
    "        w2emb = {}\n",
    "        for line in f.readlines():\n",
    "            splitted_line = line.split(\" \")\n",
    "            word = splitted_line[0]\n",
    "            embedding = list(map(float,splitted_line[1:]))\n",
    "            w2emb[word] = embedding\n",
    "        return w2emb\n",
    "\n",
    "    def look_up(self,context, w2emb) :\n",
    "        '''\n",
    "        Remplace dans le vecteur de contexte les mots par leur embedding.\n",
    "\n",
    "        Args:\n",
    "            context (list): liste de taille (size_window*2)+1\n",
    "            w2emb (dictionnary): Associe à chaque mot son embedding\n",
    "\n",
    "        Returns:\n",
    "            list : liste de taille size_embedding : BOW\n",
    "        '''\n",
    "\n",
    "        emb_size = len(list(w2emb.values())[0]) #on récupère la taille d'un embedding : 300\n",
    "        context_emb = np.zeros(emb_size)\n",
    "        for word in context :\n",
    "            if word in w2emb :\n",
    "                context_emb = np.add(context_emb, np.array(w2emb[word]))             \n",
    "        return context_emb\n",
    "    \n",
    "    def select_examples(self,examples,senses,size):\n",
    "        '''\n",
    "        Choisit des examples d'entraînement représentatifs du corpus.\n",
    "\n",
    "        Args:\n",
    "            examples (list)\n",
    "            n_senses (int): nombre de senses associés à l'instance\n",
    "            size (float): quantité des données d'entraînement considérés\n",
    "\n",
    "        Returns:\n",
    "            list: examples qui contiennent au moins un example de chaque sense\n",
    "        '''\n",
    "\n",
    "        selected_examples = []\n",
    "        \n",
    "        #Pour chaque sens, on ajoute un example associé à ce sens ,au hasard\n",
    "        for sense in senses :\n",
    "            selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),examples))))\n",
    "        \n",
    "        #On calcule ensuite le nombre d'examples qu'il reste à ajouter pour atteindre la quantité de données souhaitée\n",
    "        size_to_add = round(size*(len(examples)))-len(selected_examples)\n",
    "        \n",
    "        #On ajoute ce nombre de données (non-présentes déjà dans la liste) selectionnées au hasard\n",
    "        selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),examples)),k=size_to_add))\n",
    "        \n",
    "        return selected_examples\n",
    "    \n",
    "    def define_instance(self,instance,data_size,affichage=True):\n",
    "        \"\"\"Permet de définir l'instance et les examples utilisés. Fixe les attributs des données. Contraint le choix de classification.\n",
    "\n",
    "        Args:\n",
    "            instance (string): mot à désambiguïser\n",
    "            data_size (float): quantité de données à considérer\n",
    "            affichage (bool, optional): Defaults to True.\n",
    "        \"\"\"\n",
    "        selected_examples = self.select_examples(self.w2examples[instance],self.w2senses[instance],data_size)\n",
    "        self.selected_examples = selected_examples\n",
    "        X = [self.look_up(context,self.w2emb)for context,gold in selected_examples]\n",
    "        y = [gold for context,gold in selected_examples]\n",
    "        \n",
    "        self.selected_embs = list(zip(X,y))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        if affichage :\n",
    "            print(\"instance :\",instance)\n",
    "            print(f'{data_size*100}% des données annotées considérées')\n",
    "            print(\"nombre de données d'entraînement : \", len(self.X_train))\n",
    "            print(\"étiquettes possibles pour cette instance : \", self.w2senses[instance])\n",
    "            print(\"étiquettes présentes dans les données d'entraînement :\",Counter(self.y_train)) \n",
    "        return\n",
    "    \n",
    "    def get_sets(self):\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def get_examples(self):\n",
    "        return self.selected_examples\n",
    "    \n",
    "    def get_embs(self):\n",
    "        return self.selected_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A définir\n",
    "# chemin pour récupérer les données annotées\n",
    "data_path = \"../donnees/FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "# chemin pour récupérer les gold class\n",
    "gold_path = \"../donnees/FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "# choix de la fenêtre du contexte\n",
    "context_size = 4\n",
    "# chemin pour pouvoir faire l'opération look-up. Les embeddings sont extraits de fasttext\n",
    "embeddings_path = \"embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance : aboutir\n",
      "100% des données annotées considérées\n",
      "nombre de données d'entraînement :  20\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 18, 4: 1, 2: 1})\n",
      "\n",
      "exemple de contexte : (['et', 'grenelle', 'de', 'quai', 'aboutir', 'rue', 'de', 'la', 'croix'], 1)\n",
      "\n",
      "au format embedding :  (array([ 1.1260e-01,  1.1970e-01,  6.5350e-01, -6.0890e-01, -1.0010e-01,\n",
      "       -1.3570e-01,  7.1280e-01, -2.0900e-02, -2.0390e-01, -6.1000e-01,\n",
      "       -1.9460e-01, -3.1340e-01,  1.1800e-01,  5.8740e-01, -8.3500e-02,\n",
      "        1.8990e-01,  5.5980e-01, -2.1220e-01, -4.5650e-01,  1.0880e-01,\n",
      "        1.7000e-03, -4.7080e-01, -4.1750e-01,  3.7060e-01,  1.4750e-01,\n",
      "        1.6180e-01,  1.1130e-01, -2.0490e-01,  3.9200e-01,  2.4550e-01,\n",
      "        1.0200e-01, -2.3870e-01,  1.4240e-01, -2.2780e-01,  3.0380e-01,\n",
      "        3.9980e-01, -7.0890e-01,  3.6210e-01,  3.1320e-01,  8.0000e-02,\n",
      "       -4.2190e-01, -7.7300e-02,  1.0730e-01,  2.4880e-01, -1.3460e-01,\n",
      "        7.1650e-01,  6.2700e-02,  1.8510e-01,  8.3200e-02,  2.3370e-01,\n",
      "        1.2180e-01, -2.9380e-01,  5.9000e-02,  3.0100e-01, -7.7600e-02,\n",
      "        1.7461e+00,  2.4230e-01,  7.0000e-03,  9.8860e-01,  6.5800e-02,\n",
      "        1.1720e-01,  7.6570e-01, -2.8910e-01, -1.3113e+00, -3.6400e-02,\n",
      "        1.2200e-02, -9.6300e-01,  2.1130e-01,  9.3900e-02,  2.7400e-01,\n",
      "       -5.0320e-01, -4.9300e-02, -2.0498e+00, -2.6590e-01, -3.7730e-01,\n",
      "       -1.4430e-01, -2.2800e-01, -1.3900e-01,  1.4821e+00, -5.9400e-02,\n",
      "        3.0000e-03, -2.3480e-01, -9.5500e-01, -5.8310e+00, -2.8650e-01,\n",
      "       -6.8390e-01,  2.4500e-01, -5.2800e-02,  1.3930e-01, -1.2340e-01,\n",
      "       -6.0270e-01,  2.2520e-01, -1.2500e-01,  8.5500e-02,  4.7960e-01,\n",
      "       -9.7330e-01,  2.0940e-01, -3.7260e-01, -7.8040e-01,  2.6654e+00,\n",
      "        4.0700e-02,  2.3900e-01,  5.8900e-02,  3.8900e-01,  7.8300e-02,\n",
      "       -1.6230e-01,  2.9980e-01,  1.8720e-01,  9.2200e-02,  1.5373e+00,\n",
      "        1.6240e-01, -2.1800e-01, -7.9510e-01,  4.3380e-01, -9.4000e-03,\n",
      "        9.4600e-02,  1.3940e-01,  3.4830e-01, -2.2720e-01,  8.8100e-02,\n",
      "       -4.0770e-01,  3.5950e-01, -4.3720e-01,  1.0799e+00, -2.4340e-01,\n",
      "       -7.7300e-02,  2.0040e-01,  2.7850e-01,  7.1080e-01, -1.4990e-01,\n",
      "       -4.2700e-02,  1.8510e-01,  2.1480e-01, -1.8770e-01,  1.2170e-01,\n",
      "       -1.5930e-01, -2.0756e+00, -1.1930e-01,  2.5180e-01,  6.2760e-01,\n",
      "       -8.8600e-02, -3.9000e-03, -5.1100e-02,  1.7300e-01,  1.3850e-01,\n",
      "       -3.6400e-02, -2.6100e-02, -5.6000e-02,  8.5100e-02, -3.6260e-01,\n",
      "        1.6180e-01, -2.3420e-01, -1.9920e-01,  2.0260e-01,  4.8220e-01,\n",
      "       -1.7700e-01, -2.7800e-01, -1.2770e-01,  2.0500e-01, -2.9040e-01,\n",
      "        4.1550e-01,  4.3380e-01, -1.4680e-01, -2.5920e-01, -1.0940e-01,\n",
      "        1.0500e-01,  1.1660e-01, -2.6750e-01, -2.4000e-02, -4.5900e-01,\n",
      "        1.2737e+00,  4.7500e-02,  2.4350e-01, -9.3360e-01, -3.8900e-02,\n",
      "        2.1800e-02, -1.0100e-02, -1.0737e+00, -1.0490e-01, -3.3490e-01,\n",
      "       -3.2290e-01,  1.6410e-01,  7.8450e-01, -2.1720e-01, -1.7270e-01,\n",
      "        4.0860e-01,  1.0217e+00,  3.5720e-01, -1.3460e+00, -3.9300e-02,\n",
      "        7.2510e-01,  3.0990e-01,  4.3350e-01,  9.1040e-01, -4.0440e-01,\n",
      "       -4.5210e-01, -1.4290e-01, -2.5390e-01,  6.5000e-02,  8.3100e-02,\n",
      "        2.9370e-01, -6.3550e-01,  5.5140e-01, -2.6200e-02,  2.4170e-01,\n",
      "       -2.6800e-02,  1.1898e+00,  3.7080e-01,  1.1494e+00,  3.0280e-01,\n",
      "       -9.1010e-01, -7.2660e-01,  2.1638e+00, -3.8900e-02, -2.7100e-01,\n",
      "        4.9600e-02, -4.1840e-01,  1.2230e-01,  8.0600e-02,  2.0800e-02,\n",
      "        1.0880e-01, -1.8600e-02,  3.9730e-01,  2.6330e-01,  1.7600e-01,\n",
      "       -1.8800e-02, -1.2310e-01, -1.8440e-01,  5.1100e-02, -8.8570e-01,\n",
      "       -1.8000e-01, -1.6429e+00, -1.1410e-01,  5.7700e-02, -2.7740e-01,\n",
      "       -2.9900e-01, -6.1200e-02,  1.8420e-01,  2.4180e-01, -9.9000e-02,\n",
      "        2.6510e-01, -4.2120e-01, -3.2310e-01, -1.4990e-01,  5.8930e-01,\n",
      "        4.4390e-01,  3.0970e-01, -6.0900e-02, -3.9240e-01, -2.8530e-01,\n",
      "       -1.1770e-01, -4.8680e-01, -2.2300e-02, -9.0100e-02, -4.0960e-01,\n",
      "        3.3630e-01, -3.8500e-02,  1.8150e-01,  7.6000e-02,  4.1300e-02,\n",
      "        1.1720e-01, -3.7100e-02, -2.7580e-01,  7.0180e-01, -2.8860e-01,\n",
      "       -1.3090e-01,  2.6020e-01,  1.4160e-01,  3.7370e-01,  1.6000e-01,\n",
      "       -7.0530e-01, -4.4000e-02, -1.6760e-01, -1.3410e-01, -1.5490e-01,\n",
      "        7.0400e-02,  1.6000e-01,  2.5950e-01,  3.7000e-02, -4.2130e-01,\n",
      "       -6.9240e-01,  6.1400e-02,  1.4580e-01, -1.3240e-01, -1.6510e-01,\n",
      "        1.5600e-02,  3.3620e-01,  1.4440e-01,  4.5610e-01,  3.9000e-03,\n",
      "       -1.3290e-01,  6.0000e-03, -1.4650e-01,  1.4610e-01,  4.6200e-02,\n",
      "        8.3210e-01,  3.3380e-01,  8.3360e-01,  2.6900e-02,  6.9800e-01]), 1)\n"
     ]
    }
   ],
   "source": [
    "ext = Extractor(data_path,gold_path,embeddings_path,context_size)\n",
    "ext.define_instance(\"aboutir\",1)\n",
    "X_train, X_test, y_train, y_test = ext.get_sets()\n",
    "print(\"\\nexemple de contexte :\",ext.get_examples()[0])\n",
    "print(\"\\nau format embedding : \",ext.get_embs()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification supervisé"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit une classe Classifieur qui va nous permettre de prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifieur :\n",
    "    \n",
    "    def __init__(self,X_train, X_test, y_train, y_test) :\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test \n",
    "        \n",
    "    def classify(self,affichage=False) :\n",
    "        \"\"\"Permet de prédire et d'afficher les données de classification.\n",
    "\n",
    "        Args:\n",
    "            affichage (bool, optional): Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list: prédiction\n",
    "        \"\"\"\n",
    "        clf = MLPClassifier(random_state=1,hidden_layer_sizes=(100,)) \n",
    "        clf.fit(self.X_train, self.y_train)\n",
    "        y_pred = clf.predict(self.X_test)\n",
    "        \n",
    "        if affichage :\n",
    "            print(\"prediction :\",y_pred)\n",
    "            print(\"gold :\",self.y_test)\n",
    "            print(\"accuracy score : \", accuracy_score(y_pred,self.y_test),\"\\n\")\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : [3 3 3 3 3]\n",
      "gold : [3, 3, 3, 3, 1]\n",
      "accuracy score :  0.8 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clf = Classifieur(X_train,X_test,y_train,y_test)\n",
    "Clf.classify(affichage=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un second temps, on effectue nos tests sur plusieurs classifieurs en choisissant un pas de descente dans la quantité des données annotées considérées. Pour chaque classifieur et chaque quantitée de données considérées, on effectue n_repeat classifications pour obtenir une accuracy moyenne représentative du classifieur. Par conséquent, pour n_repeat=5 et step=0.25, nous obtiendrons pour chaque classifieur une liste d'accuracies correspondante à la moyenne des accuracies de 5 prédictions pour 100%, 75%, 50% et 25% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_accuracy(instance,data_size,n_repeat,affichage=True):\n",
    "        \"\"\"Permet de retourner une moyenne d'accuracies pour un nombre de classifications donné.\n",
    "\n",
    "        Args:\n",
    "            n_repeat (int): nombre de classifications à tester\n",
    "            affichage (bool, optional): Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            int: moyenne des accuracies computed\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        \n",
    "        for i in range(n_repeat) :\n",
    "            \n",
    "            #pour chaque test, on crée un nouveau classifieur, qui s'appuie toujours sur la même quantité de données mais avec un autre tirage\n",
    "            #par exemple, pour 75% des données, on obtiendra des accuracies différentes en fonction des tirages de ces données\n",
    "            ext.define_instance(instance,data_size,False)\n",
    "            X_train, X_test, y_train, y_test = ext.get_sets()\n",
    "            Clf = Classifieur(X_train, X_test, y_train, y_test)\n",
    "            y_pred = Clf.classify()\n",
    "            \n",
    "            accuracies.append(accuracy_score(y_pred,y_test))\n",
    "            mean = sum(accuracies)/len(accuracies)\n",
    "            \n",
    "        if affichage :\n",
    "            print(f'{data_size*100}% des données annotées considérées')\n",
    "            print(\"instance :\",instance)\n",
    "            print(\"accuracies :\",accuracies)\n",
    "            print(\"accuracy moyenne :\",mean)\n",
    "            print()\n",
    "        return mean\n",
    "\n",
    "def test_classifications(instances,step,n_repeat,affichage=False):\n",
    "            \"\"\"Permet d'obtenir une accuracy moyenne pour une certaine quantité de données considérée.\n",
    "\n",
    "            Args:\n",
    "                instances (string): mot à désambiguiser\n",
    "                step (float): pas de descente dans la quantité de données à considérer\n",
    "                n_repeat (int): nombre de classifications tests à effectuer\n",
    "\n",
    "            Returns:\n",
    "                dictionnary: associe à chaque instance sa liste d'accuracies moyenne \n",
    "            \"\"\"\n",
    "            instance2acc = {instance : [] for instance in instances}\n",
    "            data_sizes =[]\n",
    "            \n",
    "            for i in range(round(1.0/step)): #pour chaque quantité de données observée\n",
    "                \n",
    "                data_size = 1.0 - (step*float(i))\n",
    "                data_sizes.append(data_size)\n",
    "                \n",
    "                for instance in instances : #pour chaque mot ambigü sélectionné\n",
    "                    instance2acc[instance].append(get_mean_accuracy(instance,data_size,n_repeat,affichage)) #on récupère son accuracy\n",
    "            \n",
    "            tab = instance2acc.copy()\n",
    "            tab[\"data_sizes\"] = data_sizes \n",
    "            df = pd.DataFrame(tab)\n",
    "            df.set_index(\"data_sizes\")\n",
    "            print(f\"Accuracies moyennes obtenues avec {n_repeat} classifications\")\n",
    "            print(df)\n",
    "                \n",
    "            return instance2acc     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A définir\n",
    "#Nombre de classifieurs choisis au hasard à tester\n",
    "n_rand_instances = 3\n",
    "#Pas de descente dans la quantité de données considérées\n",
    "step = 0.25\n",
    "#Nombre de classifications pour un classifieur pour obtenir une accuracy moyenne\n",
    "n_repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies moyennes obtenues avec 10 classifications\n",
      "   interroger  adapter  demeurer  data_sizes\n",
      "0    0.740000     0.78      0.58        1.00\n",
      "1    0.625000     0.80      0.55        0.75\n",
      "2    0.466667     0.50      0.35        0.50\n",
      "3    0.300000     0.50      0.00        0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'interroger': [0.74, 0.625, 0.4666666666666666, 0.3],\n",
       " 'adapter': [0.78, 0.8, 0.5000000000000001, 0.5],\n",
       " 'demeurer': [0.5799999999999998, 0.55, 0.35, 0.0]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = random.choices(list(ext.w2examples.keys()),k=n_rand_instances)\n",
    "test_classifications(instances,step,n_repeat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification semi-supervisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "                    si example = ([1.9, 2.3, 0.6], 1),\n",
    "                    - le contexte avec le mot à désambiguiser et son lemme est \n",
    "                      l'embedding [1.9, 2.3, 0.6]\n",
    "                    - le numéro de sens est 1\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.examples = list(examples)\n",
    "        # transforme les embeddings en tensors\n",
    "        self.tensors_examples = [example[0] for example in self.examples]\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données\n",
    "        self.k = self.nb_senses()\n",
    "        # initialisation de centroids : pour chaque sens, un example est pris au hasard\n",
    "        self.tensors_centroids = [random.choice(example) \n",
    "                                  for example in self.examples_of_same_sense().values()]\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(examples))\n",
    "\n",
    "    def nb_senses(self):\n",
    "        '''\n",
    "        Renvoie le nombre de sens existants dans un ensemble d'examples\n",
    "        '''\n",
    "\n",
    "        known_senses = []\n",
    "        # pour chaque exemple\n",
    "        for example in self.examples:\n",
    "            # si le sens attendu (gold class) n'a pas encore été rencontré\n",
    "            if example[1] not in known_senses:\n",
    "                # l'ajoute à la liste des sens possibles\n",
    "                known_senses.append(example[1])\n",
    "        # renvoie le nombre de sens\n",
    "        return len(known_senses)\n",
    "    \n",
    "    def examples_of_same_sense(self):\n",
    "        '''\n",
    "        Regroupe les contextes des examples dans un dictionnaire triés selon le \n",
    "        sens du mot à désambiguiser\n",
    "        '''\n",
    "\n",
    "        # clé : numéro du sens\n",
    "        # valeur : liste de contextes avec ce sens en gold class\n",
    "        sense2examples = {}\n",
    "        # pour chaque example\n",
    "        for example in self.examples:\n",
    "            # si sa gold class n'a pas été déjà rencontrée\n",
    "            if example[1] not in sense2examples:\n",
    "                # ajoute une clé pour cette gold class\n",
    "                sense2examples[example[1]] = []\n",
    "            # ajoute le contexte au dictionnaire correspondant au sens utilisé\n",
    "            sense2examples[example[1]].append(example[0])\n",
    "\n",
    "        return sense2examples\n",
    "    \n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        while diff:\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "\n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = 0\n",
    "                    for k in range(len(tensor_example)):\n",
    "                        d += (tensor_centroid[k].item() - tensor_example[k].item())**2\n",
    "                    d = np.sqrt(d)\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "            # CALCUL DES NOUVEAUX CENTROIDES\n",
    "\n",
    "            # calcul des nouveaux centroides en utilisant le point au milieu de tous les\n",
    "            # autres points du même cluster\n",
    "            new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean()\n",
    "            # transforme ces nouveaux centroides en tensors\n",
    "            tensors_new_centroids = []\n",
    "            for i in range(len(new_centroids.index)):\n",
    "                colums = []\n",
    "                for j in range(len(new_centroids.columns)):\n",
    "                    colums.append(int(new_centroids.iat[i,j]))\n",
    "                tensors_new_centroids.append(torch.tensor(colums))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "equal(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m examples \u001b[39m=\u001b[39m ext\u001b[39m.\u001b[39mget_embs()\n\u001b[0;32m      2\u001b[0m k_means \u001b[39m=\u001b[39m K_Means(examples)\n\u001b[1;32m----> 3\u001b[0m k_means\u001b[39m.\u001b[39;49mlearn_clusters()\n",
      "Cell \u001b[1;32mIn[126], line 122\u001b[0m, in \u001b[0;36mK_Means.learn_clusters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m# pour chaque centroide\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors_centroids)):\n\u001b[0;32m    121\u001b[0m     \u001b[39m# si l'ancien centroide et le nouveau ne sont pas les mêmes\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m(torch\u001b[39m.\u001b[39;49mequal(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors_centroids[i], tensors_new_centroids[i])):\n\u001b[0;32m    123\u001b[0m         count_diff \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[39m# met à jour le centroide\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: equal(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "examples = ext.get_embs()\n",
    "k_means = K_Means(examples)\n",
    "k_means.learn_clusters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on écrit encore des trucs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
