{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook permet d'effectuer des tests de comparaison des performances d'apprentissage supervisée et semi-supervisée pour la tâche de Word Sense Disambiguation. Nous développerons deux méthodes de classification : un MLP pour la classification supervisée et un constrained K-means pour la classification semi-supervisée. Nous effectuerons plusieurs tests en considérant plusieurs mots à désambiguiser pour lesquels nous évaluerons les performances de ces deux méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification supervisé"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit une classe Classifieur qui va nous permettre d'accéder aux différents classifieurs (un classifieur pour un mot ambigü).\n",
    "\n",
    "Cette classe possède :  \n",
    "        1 : une méthode pour extraire les données d'entraînement et de test   \n",
    "        2 : une méthode pour extraire les embeddings à partir d'un fichier crée en amont, qui ne regroupe que les embeddings nécéssaires à notre jeu de données  \n",
    "        3 : une méthode pour effectuer l'étape de look-up avant la classifcation en elle-même  \n",
    "        4 : une méthode pour sélectionner des données représentatives lorsqu'on ne considère pas toutes les données annotées i.e. chaque étiquette présente dans les données est présente au moins une fois dans le set d'entraînement \n",
    "        5 : une méthode pour afficher et évaluer la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifieur :\n",
    "    \n",
    "    def __init__(self,data_path,gold_path,embeddings_path,context_size):\n",
    "        \n",
    "        #récupération des données XML\n",
    "        tree = ET.parse(data_path)\n",
    "        data_file = tree.getroot()[0]\n",
    "\n",
    "        #récupération des données .txt\n",
    "        gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "        \n",
    "        self.w2examples, self.w2senses = self.extract_examples_and_senses(data_file,gold_file,context_size)\n",
    "        self.w2emb = self.extract_embeddings(embeddings_path)\n",
    "    \n",
    "    def extract_examples_and_senses(self,data_file, gold_file, context_size):\n",
    "        \"\"\"Extract the data from the files.\n",
    "\n",
    "        Args:\n",
    "            data_file (Element): Sentences\n",
    "            gold_file (TextIOWrapper): Golds keys\n",
    "\n",
    "        Returns:\n",
    "            dictionary: associates the list of context vectors corresponding to the instance\n",
    "            dictionary : associates to the word each senses\n",
    "        '''\n",
    "    \n",
    "        w2examples={}\n",
    "        w2senses = defaultdict(set)\n",
    "        \n",
    "        for (sentence,gold_line) in zip(data_file,gold_file.readlines()) :\n",
    "            \n",
    "            #pour chaque phrase, on initialise deux listes qui permettront de respecter les tailles des contextes (+10,-10)\n",
    "            context_before = []\n",
    "            context_after = []\n",
    "            context = []\n",
    "            \n",
    "            #on boucle sur les mots de la phrase pour construire les listes\n",
    "            #on cherche l'instance et on repart en arrière pour constuire le contexte avant\n",
    "            i_instance = 0\n",
    "            while sentence[i_instance].tag != \"instance\" : \n",
    "                i_instance+=1\n",
    "            \n",
    "            instance = sentence[i_instance].attrib[\"lemma\"].lower()\n",
    "            \n",
    "            if instance not in w2examples : \n",
    "                w2examples[instance] = []\n",
    "            \n",
    "            #on vérifie la longueur des phrases pour ne pas soulever d'erreur\n",
    "            \n",
    "            #context_before \n",
    "            \n",
    "            #si le contexte avant l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance-1 to i_instance-5\n",
    "            if (len(sentence[:i_instance])>=context_size) :\n",
    "                    for i in range(1,context_size+1) :\n",
    "                        context_before.append(sentence[i_instance-i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(1,len(sentence[:i_instance])+1) :\n",
    "                    context_before.append(sentence[i_instance-i].text.lower())\n",
    "\n",
    "            #context_after\n",
    "            \n",
    "            #si le contexte après l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance+1 to i_instance+11\n",
    "            if(len(sentence[i_instance+1:])>= context_size) :\n",
    "                for i in range(i_instance+1,i_instance+(context_size+1)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(i_instance+1,len(sentence)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #une fois les listes constituées, on ajoute les balises de début et de fin de phrase si nécessaire\n",
    "            for i in range(context_size-len(context_before)) :\n",
    "                context_before.append(\"<d>\")\n",
    "                \n",
    "            for i in range(context_size-len(context_after)) :\n",
    "                context_after.append(\"<f>\")\n",
    "                \n",
    "            #le vecteur sera une concaténation des contextes d'avant et d'après\n",
    "            context = context_before\n",
    "            context.append(instance)\n",
    "            context.extend(context_after)\n",
    "                \n",
    "            #on récupère ensuite le nombre associé au sens pour constuire l'exemple + ajouter au dictionnaire w2sense\n",
    "            gold = int((re.findall(\"ws_[0-9]\",gold_line)[0]).replace(\"ws_\",\"\"))\n",
    "            \n",
    "            w2senses[instance].add(gold)\n",
    "            w2examples[instance].append((context,gold))\n",
    "            \n",
    "        return w2examples,w2senses\n",
    "    \n",
    "    def extract_embeddings(self,path_embeddings) :\n",
    "        '''\n",
    "        Récupère les embeddings dans le fichier générée.\n",
    "\n",
    "        Args:\n",
    "            path_embeddings (string)\n",
    "\n",
    "        Returns:\n",
    "            dictionnary: Associe à chaque mot son embedding\n",
    "        '''\n",
    "\n",
    "        f = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "        #On récupère dans le fichier crée les embeddings pour créer un dictionnaire\n",
    "        w2emb = {}\n",
    "        for line in f.readlines():\n",
    "            splitted_line = line.split(\" \")\n",
    "            word = splitted_line[0]\n",
    "            embedding = list(map(float,splitted_line[1:]))\n",
    "            w2emb[word] = embedding\n",
    "        return w2emb\n",
    "\n",
    "    def look_up(self,context, w2emb) :\n",
    "        '''\n",
    "        Remplace dans le vecteur de contexte les mots par leur embedding.\n",
    "\n",
    "        Args:\n",
    "            context (list): liste de taille (size_window*2)+1\n",
    "            w2emb (dictionnary): Associe à chaque mot son embedding\n",
    "\n",
    "        Returns:\n",
    "            list : liste de taille size_embedding : BOW\n",
    "        '''\n",
    "\n",
    "        emb_size = len(list(w2emb.values())[0]) #on récupère la taille d'un embedding : 300\n",
    "        context_emb = np.zeros(emb_size)\n",
    "        for word in context :\n",
    "            if word in w2emb :\n",
    "                context_emb = np.add(context_emb, np.array(w2emb[word]))             \n",
    "        return context_emb\n",
    "    \n",
    "    def select_examples(self,examples,senses,size):\n",
    "        '''\n",
    "        Choisit des examples d'entraînement représentatifs du corpus.\n",
    "\n",
    "        Args:\n",
    "            examples (list)\n",
    "            n_senses (int): nombre de senses associés à l'instance\n",
    "            size (float): quantité des données d'entraînement considérés\n",
    "\n",
    "        Returns:\n",
    "            list: examples qui contiennent au moins un example de chaque sense\n",
    "        '''\n",
    "\n",
    "        selected_examples = []\n",
    "        \n",
    "        #Pour chaque sens, on ajoute un example associé à ce sens ,au hasard\n",
    "        for sense in senses :\n",
    "            selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),examples))))\n",
    "        \n",
    "        #On calcule ensuite le nombre d'examples qu'il reste à ajouter pour atteindre la quantité de données souhaitée\n",
    "        size_to_add = round(size*(len(examples)))-len(selected_examples)\n",
    "        \n",
    "        #On ajoute ce nombre de données (non-présentes déjà dans la liste) selectionnées au hasard\n",
    "        selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),examples)),k=size_to_add))\n",
    "        \n",
    "        return selected_examples\n",
    "\n",
    "    def classify(self,instance,data_size,affichage=True) :\n",
    "        \"\"\"Permet d'afficher les données de classification et de prédire.\n",
    "\n",
    "        Args:\n",
    "            instance (string): mot anbigü à désambiguïser\n",
    "            data_size (float): quantité de données à considérer\n",
    "            affichage (bool, optional): affichage ou non des données de classification. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        clf = MLPClassifier(random_state=1,hidden_layer_sizes=(100,)) \n",
    "        \n",
    "        selected_examples = self.select_examples(self.w2examples[instance],self.w2senses[instance],data_size)\n",
    "        X = [self.look_up(context,self.w2emb)for context,gold in selected_examples]\n",
    "        y = [gold for context,gold in selected_examples]\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test \n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        if affichage :\n",
    "            print(\"instance :\",instance)\n",
    "            print(f'{data_size*100}% des données annotées considérées')\n",
    "            print(\"nombre de données d'entraînement : \", len(X_train))\n",
    "            print(\"étiquettes possibles pour cette instance : \", self.w2senses[instance])\n",
    "            print(\"étiquettes présentes dans les données d'entraînement :\",Counter(list(zip(*selected_examples))[1]))\n",
    "            print(\"prédiction :\", y_pred)\n",
    "            print(\"gold :\",y_test)\n",
    "            print(\"accuracy score : \", accuracy_score(y_pred,y_test),\"\\n\")\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_mean_accuracy(self,instance,data_size,n_repeat):\n",
    "        \"\"\"Permet d'effectuer différentes classifications et de rendre une moyenne d'accuracies.\n",
    "\n",
    "        Args:\n",
    "            instance (string): mot à désambiguiser\n",
    "            data_size (float): quantité de données à considérer\n",
    "            n_repeat (int): nombre de classifications tests à effectuer\n",
    "\n",
    "        Returns:\n",
    "            int: moyenne des accuracies obtenues\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for i in range(n_repeat) :\n",
    "            y_pred = self.classify(instance,data_size,False)\n",
    "            accuracies.append(accuracy_score(y_pred,self.y_test))\n",
    "        return sum(accuracies)/len(accuracies)\n",
    "\n",
    "    def test_classifications(self,instances,step,n_repeat):\n",
    "        \"\"\"Permet d'obtenir une accuracy moyenne pour une certaine quantité de données considérée.\n",
    "\n",
    "        Args:\n",
    "            instances (string): mot à désambiguiser\n",
    "            step (float): pas de descente dans la quantité de données à considérer\n",
    "            n_repeat (int): nombre de classifications tests à effectuer\n",
    "\n",
    "        Returns:\n",
    "            dictionnary: associe à chaque instance sa liste d'accuracies moyenne \n",
    "        \"\"\"\n",
    "        \n",
    "        instance2acc = {instance : [] for instance in instances}\n",
    "        \n",
    "        for i in range(round(1.0/step)):\n",
    "            \n",
    "            data_size = 1.0 - (step*float(i))\n",
    "\n",
    "            for instance in instances :\n",
    "                \n",
    "                instance2acc[instance].append(self.get_mean_accuracy(instance,data_size,n_repeat))\n",
    "                \n",
    "        return instance2acc     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer le Classifieur, il faut définir les chemins des données, le chemin du fichier stockant les embeddings et la taille du contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A définir\n",
    "# chemin pour récupérer les données annotées\n",
    "data_path = \"../donnees/FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "# chemin pour récupérer les gold class\n",
    "gold_path = \"../donnees/FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "# choix de la fenêtre du contexte\n",
    "context_size = 4\n",
    "# chemin pour pouvoir faire l'opération look-up. Les embeddings sont extraits de fasttext\n",
    "embeddings_path = \"embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifieur(data_path,gold_path,embeddings_path,context_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue un premier test sur le premier mot ambigü des données d'entraînement : \"aboutir\". On choisit de considérer 100% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance : aboutir\n",
      "100% des données annotées considérées\n",
      "nombre de données d'entraînement :  20\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 22, 1: 1, 2: 1, 4: 1})\n",
      "prédiction : [3 3 3 3 3]\n",
      "gold : [3, 4, 3, 3, 3]\n",
      "accuracy score :  0.8 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classify(\"aboutir\",1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un second temps, on effectue nos tests sur plusieurs classifieurs en choisissant un pas de descente dans la quantité des données annotées considérées. Pour chaque classifieur et chaque quantitée de données considérées, on effectue n_repeat classifications pour obtenir une accuracy moyenne représentative du classifieur. Par conséquent, pour n_repeat=5 et step=0.25, nous obtiendrons pour chaque classifieur une liste d'accuracies correspondante à la moyenne des accuracies de 5 prédictions pour 100%, 75%, 50% et 25% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A définir\n",
    "n_rand_instances = 3\n",
    "step = 0.25\n",
    "n_repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = random.choices(list(clf.w2examples.keys()),k=n_rand_instances)\n",
    "tab = clf.test_classifications(instances,step,n_repeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggraver</th>\n",
       "      <th>entraîner</th>\n",
       "      <th>traduire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_sizes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100.0%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75.0%</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0%</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            aggraver  entraîner  traduire\n",
       "data_sizes                               \n",
       "100.0%         1.000       0.56     0.700\n",
       "75.0%          0.925       0.65     0.625\n",
       "50.0%          1.000       0.40     0.600\n",
       "25.0%          0.600       0.20     0.200"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sizes = [str(step*float(i)*100)+\"%\" for i in range(1,int(1/0.25)+1)]\n",
    "data_sizes.reverse()\n",
    "tab[\"data_sizes\"] = data_sizes \n",
    "table1 = pd.DataFrame(tab)\n",
    "table1.set_index(\"data_sizes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on écrit d'autres trucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\__init__.py:1465\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m library\n\u001b[0;32m   1464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   1467\u001b[0m \u001b[39m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mTORCH_CUDA_SANITIZER\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_meta_registrations.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m \u001b[39mimport\u001b[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mimport\u001b[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_decomp\\__init__.py:169\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m decompositions\n\u001b[0;32m    168\u001b[0m \u001b[39m# populate the table\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecompositions\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_refs\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# This list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_decomp\\decompositions.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mprims\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_prims\\__init__.py:33\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     check,\n\u001b[0;32m     19\u001b[0m     Dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     type_to_dtype,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m backwards_not_supported\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeTensor, FakeTensorMode\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytree\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_subclasses\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     DynamicOutputShapeException,\n\u001b[0;32m      5\u001b[0m     FakeTensor,\n\u001b[0;32m      6\u001b[0m     FakeTensorMode,\n\u001b[0;32m      7\u001b[0m     UnsupportedFakeTensorException,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossRefFakeMode\n\u001b[0;32m     12\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensorMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCrossRefFakeMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mweakref\u001b[39;00m \u001b[39mimport\u001b[39;00m ReferenceType\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_guards\u001b[39;00m \u001b[39mimport\u001b[39;00m Source\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     elementwise_dtypes,\n\u001b[0;32m     17\u001b[0m     ELEMENTWISE_TYPE_PROMOTION_KIND,\n\u001b[0;32m     18\u001b[0m     is_float_dtype,\n\u001b[0;32m     19\u001b[0m     is_integer_dtype,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_guards.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# TODO(voz): Stolen pattern, not sure why this is the case,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# but mypy complains.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msympy\u001b[39;00m  \u001b[39m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mNo sympy found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\__init__.py:51\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munrecognized value for SYMPY_DEBUG: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     48\u001b[0m                            debug_str)\n\u001b[0;32m     49\u001b[0m SYMPY_DEBUG \u001b[39m=\u001b[39m __sympy_debug()  \u001b[39m# type: bool\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (sympify, SympifyError, cacheit, Basic, Atom,\n\u001b[0;32m     52\u001b[0m         preorder_traversal, S, Expr, AtomicExpr, UnevaluatedExpr, Symbol,\n\u001b[0;32m     53\u001b[0m         Wild, Dummy, symbols, var, Number, Float, Rational, Integer,\n\u001b[0;32m     54\u001b[0m         NumberSymbol, RealNumber, igcd, ilcm, seterr, E, I, nan, oo, pi, zoo,\n\u001b[0;32m     55\u001b[0m         AlgebraicNumber, comp, mod_inverse, Pow, integer_nthroot, integer_log,\n\u001b[0;32m     56\u001b[0m         Mul, prod, Add, Mod, Rel, Eq, Ne, Lt, Le, Gt, Ge, Equality,\n\u001b[0;32m     57\u001b[0m         GreaterThan, LessThan, Unequality, StrictGreaterThan, StrictLessThan,\n\u001b[0;32m     58\u001b[0m         vectorize, Lambda, WildFunction, Derivative, diff, FunctionClass,\n\u001b[0;32m     59\u001b[0m         Function, Subs, expand, PoleError, count_ops, expand_mul, expand_log,\n\u001b[0;32m     60\u001b[0m         expand_func, expand_trig, expand_complex, expand_multinomial, nfloat,\n\u001b[0;32m     61\u001b[0m         expand_power_base, expand_power_exp, arity, PrecisionExhausted, N,\n\u001b[0;32m     62\u001b[0m         evalf, Tuple, Dict, gcd_terms, factor_terms, factor_nc, evaluate,\n\u001b[0;32m     63\u001b[0m         Catalan, EulerGamma, GoldenRatio, TribonacciConstant, bottom_up, use,\n\u001b[0;32m     64\u001b[0m         postorder_traversal, default_sort_key, ordered)\n\u001b[0;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m \u001b[39mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[0;32m     67\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[0;32m     68\u001b[0m         true, false, satisfiable)\n\u001b[0;32m     70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massumptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[0;32m     71\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbasic\u001b[39;00m \u001b[39mimport\u001b[39;00m Basic, Atom\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msingleton\u001b[39;00m \u001b[39mimport\u001b[39;00m S\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexpr\u001b[39;00m \u001b[39mimport\u001b[39;00m Expr, AtomicExpr, UnevaluatedExpr\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msymbol\u001b[39;00m \u001b[39mimport\u001b[39;00m Symbol, Wild, Dummy, symbols, var\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number, Float, Rational, Integer, NumberSymbol, \\\n\u001b[0;32m     12\u001b[0m     RealNumber, igcd, ilcm, seterr, E, I, nan, oo, pi, zoo, \\\n\u001b[0;32m     13\u001b[0m     AlgebraicNumber, comp, mod_inverse\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\expr.py:4128\u001b[0m\n\u001b[0;32m   4124\u001b[0m                 \u001b[39mreturn\u001b[39;00m (i,)\n\u001b[0;32m   4125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 4128\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmul\u001b[39;00m \u001b[39mimport\u001b[39;00m Mul\n\u001b[0;32m   4129\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madd\u001b[39;00m \u001b[39mimport\u001b[39;00m Add\n\u001b[0;32m   4130\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpower\u001b[39;00m \u001b[39mimport\u001b[39;00m Pow\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\mul.py:2131\u001b[0m\n\u001b[0;32m   2127\u001b[0m         \u001b[39mreturn\u001b[39;00m e\n\u001b[0;32m   2128\u001b[0m     \u001b[39mreturn\u001b[39;00m bottom_up(e, do)\n\u001b[1;32m-> 2131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Rational\n\u001b[0;32m   2132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpower\u001b[39;00m \u001b[39mimport\u001b[39;00m Pow\n\u001b[0;32m   2133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madd\u001b[39;00m \u001b[39mimport\u001b[39;00m Add, _unevaluated_Add\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\numbers.py:1480\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this is here to work nicely in Sage\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m RealNumber \u001b[39m=\u001b[39m Float\n\u001b[1;32m-> 1480\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRational\u001b[39;00m(Number):\n\u001b[0;32m   1481\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Represents rational numbers (p/q) of any size.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[0;32m   1483\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39m    sympy.core.sympify.sympify, sympy.simplify.simplify.nsimplify\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m     is_real \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\numbers.py:1586\u001b[0m, in \u001b[0;36mRational\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1581\u001b[0m q: \u001b[39mint\u001b[39m\n\u001b[0;32m   1583\u001b[0m is_Rational \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m \u001b[39m@cacheit\u001b[39;49m\n\u001b[1;32m-> 1586\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, p, q\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, gcd\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m):\n\u001b[0;32m   1587\u001b[0m     \u001b[39mif\u001b[39;49;00m q \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[0;32m   1588\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(p, Rational):\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sympy\\core\\cache.py:68\u001b[0m, in \u001b[0;36m__cacheit.<locals>.func_wrapper\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_wrapper\u001b[39m(func):\n\u001b[0;32m     65\u001b[0m     cfunc \u001b[39m=\u001b[39m lru_cache(maxsize, typed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(func)\n\u001b[0;32m     67\u001b[0m     \u001b[39m@wraps\u001b[39;49m(func)\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39mwrapper\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[0;32m     69\u001b[0m         \u001b[39mtry\u001b[39;49;00m:\n\u001b[0;32m     70\u001b[0m             retval \u001b[39m=\u001b[39;49m cfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anais\\AppData\\Local\\Programs\\Python\\Python39\\lib\\functools.py:52\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[1;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m assigned:\n\u001b[0;32m     51\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(wrapped, attr)\n\u001b[0;32m     53\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "                    si example = ([1.9, 2.3, 0.6], 1),\n",
    "                    - le contexte avec le mot à désambiguiser et son lemme est \n",
    "                      l'embedding [1.9, 2.3, 0.6]\n",
    "                    - le numéro de sens est 1\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.examples = list(examples)\n",
    "        # transforme les embeddings en tensors\n",
    "        self.tensors_examples = [example[0] for example in self.examples]\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données\n",
    "        self.k = self.nb_senses()\n",
    "        # initialisation de centroids : pour chaque sens, un example est pris au hasard\n",
    "        self.tensors_centroids = [random.choice(example) \n",
    "                                  for example in self.examples_of_same_sense().values()]\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(examples))\n",
    "\n",
    "    def nb_senses(self):\n",
    "        '''\n",
    "        Renvoie le nombre de sens existants dans un ensemble d'examples\n",
    "        '''\n",
    "\n",
    "        known_senses = []\n",
    "        # pour chaque exemple\n",
    "        for example in self.examples:\n",
    "            # si le sens attendu (gold class) n'a pas encore été rencontré\n",
    "            if example[1] not in known_senses:\n",
    "                # l'ajoute à la liste des sens possibles\n",
    "                known_senses.append(example[1])\n",
    "        # renvoie le nombre de sens\n",
    "        return len(known_senses)\n",
    "    \n",
    "    def examples_of_same_sense(self):\n",
    "        '''\n",
    "        Regroupe les contextes des examples dans un dictionnaire triés selon le \n",
    "        sens du mot à désambiguiser\n",
    "        '''\n",
    "\n",
    "        # clé : numéro du sens\n",
    "        # valeur : liste de contextes avec ce sens en gold class\n",
    "        sense2examples = {}\n",
    "        # pour chaque example\n",
    "        for example in self.examples:\n",
    "            # si sa gold class n'a pas été déjà rencontrée\n",
    "            if example[1] not in sense2examples:\n",
    "                # ajoute une clé pour cette gold class\n",
    "                sense2examples[example[1]] = []\n",
    "            # ajoute le contexte au dictionnaire correspondant au sens utilisé\n",
    "            sense2examples[example[1]].append(example[0])\n",
    "\n",
    "        return sense2examples\n",
    "    \n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        while diff:\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "\n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = 0\n",
    "                    for k in range(len(tensor_example)):\n",
    "                        d += (tensor_centroid[k].item() - tensor_example[k].item())**2\n",
    "                    d = np.sqrt(d)\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "            # CALCUL DES NOUVEAUX CENTROIDES\n",
    "\n",
    "            # calcul des nouveaux centroides en utilisant le point au milieu de tous les\n",
    "            # autres points du même cluster\n",
    "            new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean()\n",
    "            # transforme ces nouveaux centroides en tensors\n",
    "            tensors_new_centroids = []\n",
    "            for i in range(len(new_centroids.index)):\n",
    "                colums = []\n",
    "                for j in range(len(new_centroids.columns)):\n",
    "                    colums.append(int(new_centroids.iat[i,j]))\n",
    "                tensors_new_centroids.append(torch.tensor(colums))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on écrit encore des trucs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
