{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSD supervisé : MLP avec sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin pour récupérer les données annotées\n",
    "data_path = \"../donnees/FSE-1.1-191210/FSE-1.1.data.xml\"\n",
    "# chemin pour récupérer les gold class\n",
    "gold_path = \"../donnees/FSE-1.1-191210/FSE-1.1.gold.key.txt\"\n",
    "# choix de la fenêtre du contexte\n",
    "context_size = 4\n",
    "# chemin pour pouvoir faire l'opération look-up. Les embeddings sont extraits de fasttext\n",
    "embeddings_path = \"embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance : aboutir\n",
      "100.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  20\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 22, 1: 1, 2: 1, 4: 1})\n",
      "prédiction : [3 3 3 3 3]\n",
      "gold : [3, 3, 4, 3, 3]\n",
      "accuracy score :  0.8 \n",
      "\n",
      "\n",
      "instance : investir\n",
      "100.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  20\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 6}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 8, 4: 7, 1: 5, 2: 4, 6: 1})\n",
      "prédiction : [3 3 3 4 4]\n",
      "gold : [3, 3, 6, 1, 2]\n",
      "accuracy score :  0.4 \n",
      "\n",
      "\n",
      "instance : traduire\n",
      "100.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  20\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 5}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({1: 18, 2: 4, 3: 1, 4: 1, 5: 1})\n",
      "prédiction : [1 1 1 2 1]\n",
      "gold : [1, 1, 1, 2, 1]\n",
      "accuracy score :  1.0 \n",
      "\n",
      "\n",
      "instance : aboutir\n",
      "75.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  15\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 16, 1: 1, 2: 1, 4: 1})\n",
      "prédiction : [3 3 3 3]\n",
      "gold : [4, 3, 3, 3]\n",
      "accuracy score :  0.75 \n",
      "\n",
      "\n",
      "instance : investir\n",
      "75.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  15\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 6}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({4: 8, 3: 6, 1: 2, 2: 2, 6: 1})\n",
      "prédiction : [3 4 4 3]\n",
      "gold : [3, 2, 4, 3]\n",
      "accuracy score :  0.75 \n",
      "\n",
      "\n",
      "instance : traduire\n",
      "75.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  15\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 5}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({1: 12, 2: 4, 3: 1, 4: 1, 5: 1})\n",
      "prédiction : [1 2 1 2]\n",
      "gold : [1, 2, 1, 1]\n",
      "accuracy score :  0.75 \n",
      "\n",
      "\n",
      "instance : aboutir\n",
      "50.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  9\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 9, 1: 1, 2: 1, 4: 1})\n",
      "prédiction : [3 3 3]\n",
      "gold : [3, 3, 3]\n",
      "accuracy score :  1.0 \n",
      "\n",
      "\n",
      "instance : investir\n",
      "50.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  9\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 6}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 5, 4: 3, 1: 2, 2: 1, 6: 1})\n",
      "prédiction : [3 3 3]\n",
      "gold : [4, 4, 3]\n",
      "accuracy score :  0.3333333333333333 \n",
      "\n",
      "\n",
      "instance : traduire\n",
      "50.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  9\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 5}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({1: 7, 2: 2, 3: 1, 4: 1, 5: 1})\n",
      "prédiction : [4 1 1]\n",
      "gold : [1, 3, 1]\n",
      "accuracy score :  0.3333333333333333 \n",
      "\n",
      "\n",
      "instance : aboutir\n",
      "25.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  4\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({3: 3, 1: 1, 2: 1, 4: 1})\n",
      "prédiction : [3 3]\n",
      "gold : [1, 3]\n",
      "accuracy score :  0.5 \n",
      "\n",
      "\n",
      "instance : investir\n",
      "25.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  4\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 6}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({4: 2, 1: 1, 2: 1, 3: 1, 6: 1})\n",
      "prédiction : [4 3]\n",
      "gold : [2, 1]\n",
      "accuracy score :  0.0 \n",
      "\n",
      "\n",
      "instance : traduire\n",
      "25.0% des données annotées considérées\n",
      "nombre de données d'entraînement :  4\n",
      "étiquettes possibles pour cette instance :  {1, 2, 3, 4, 5}\n",
      "étiquettes présentes dans les données d'entraînement : Counter({1: 2, 2: 1, 3: 1, 4: 1, 5: 1})\n",
      "prédiction : [5 5]\n",
      "gold : [1, 4]\n",
      "accuracy score :  0.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifieur :\n",
    "    \n",
    "    def __init__(self,data_path,gold_path,embeddings_path):\n",
    "        \n",
    "        #récupération des données XML\n",
    "        tree = ET.parse(data_path)\n",
    "        data_file = tree.getroot()[0]\n",
    "\n",
    "        #récupération des données .txt\n",
    "        gold_file = open(gold_path, \"r\",encoding=\"utf-8\")\n",
    "        \n",
    "        self.w2examples, self.w2senses = self.extract_examples_and_senses(data_file,gold_file)\n",
    "        self.w2emb = self.extract_embeddings(embeddings_path)\n",
    "    \n",
    "    def extract_examples_and_senses(self,data_file, gold_file):\n",
    "        '''\n",
    "        Extract the data from the files.\n",
    "\n",
    "        Args:\n",
    "            data_file (Element): Sentences\n",
    "            gold_file (TextIOWrapper): Golds keys\n",
    "\n",
    "        Returns:\n",
    "            dictionary: associates the list of context vectors corresponding to the instance\n",
    "            dictionary : associates to the word each senses\n",
    "        '''\n",
    "    \n",
    "        w2examples={}\n",
    "        w2senses = defaultdict(set)\n",
    "        \n",
    "        for (sentence,gold_line) in zip(data_file,gold_file.readlines()) :\n",
    "            \n",
    "            #pour chaque phrase, on initialise deux listes qui permettront de respecter les tailles des contextes (+10,-10)\n",
    "            context_before = []\n",
    "            context_after = []\n",
    "            context = []\n",
    "            \n",
    "            #on boucle sur les mots de la phrase pour construire les listes\n",
    "            #on cherche l'instance et on repart en arrière pour constuire le contexte avant\n",
    "            i_instance = 0\n",
    "            while sentence[i_instance].tag != \"instance\" : \n",
    "                i_instance+=1\n",
    "            \n",
    "            instance = sentence[i_instance].attrib[\"lemma\"].lower()\n",
    "            \n",
    "            if instance not in w2examples : \n",
    "                w2examples[instance] = []\n",
    "            \n",
    "            #on vérifie la longueur des phrases pour ne pas soulever d'erreur\n",
    "            \n",
    "            #context_before \n",
    "            \n",
    "            #si le contexte avant l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance-1 to i_instance-5\n",
    "            if (len(sentence[:i_instance])>=context_size) :\n",
    "                    for i in range(1,context_size+1) :\n",
    "                        context_before.append(sentence[i_instance-i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(1,len(sentence[:i_instance])+1) :\n",
    "                    context_before.append(sentence[i_instance-i].text.lower())\n",
    "\n",
    "            #context_after\n",
    "            \n",
    "            #si le contexte après l'instance est supérieur ou égale à la taille du contexte choisie\n",
    "            #on ajoute à la liste chaque mot aux index from i-instance+1 to i_instance+11\n",
    "            if(len(sentence[i_instance+1:])>= context_size) :\n",
    "                for i in range(i_instance+1,i_instance+(context_size+1)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #sinon, on ajoute à la liste tous les mots et on ajoutera des balises pour compléter\n",
    "            else :\n",
    "                for i in range(i_instance+1,len(sentence)):\n",
    "                    context_after.append(sentence[i].text.lower())\n",
    "            \n",
    "            #une fois les listes constituées, on ajoute les balises de début et de fin de phrase si nécessaire\n",
    "            for i in range(context_size-len(context_before)) :\n",
    "                context_before.append(\"<d>\")\n",
    "                \n",
    "            for i in range(context_size-len(context_after)) :\n",
    "                context_after.append(\"<f>\")\n",
    "                \n",
    "            #le vecteur sera une concaténation des contextes d'avant et d'après\n",
    "            context = context_before\n",
    "            context.append(instance)\n",
    "            context.extend(context_after)\n",
    "                \n",
    "            #on récupère ensuite le nombre associé au sens pour constuire l'exemple + ajouter au dictionnaire w2sense\n",
    "            gold = int((re.findall(\"ws_[0-9]\",gold_line)[0]).replace(\"ws_\",\"\"))\n",
    "            \n",
    "            w2senses[instance].add(gold)\n",
    "            w2examples[instance].append((context,gold))\n",
    "            \n",
    "        return w2examples,w2senses\n",
    "    \n",
    "    def extract_embeddings(self,path_embeddings) :\n",
    "        '''\n",
    "        Récupère les embeddings dans le fichier générée.\n",
    "\n",
    "        Args:\n",
    "            path_embeddings (string)\n",
    "\n",
    "        Returns:\n",
    "            dictionnary: Associe à chaque mot son embedding\n",
    "        '''\n",
    "\n",
    "        f = open(path_embeddings , \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "        #On récupère dans le fichier crée les embeddings pour créer un dictionnaire\n",
    "        w2emb = {}\n",
    "        for line in f.readlines():\n",
    "            splitted_line = line.split(\" \")\n",
    "            word = splitted_line[0]\n",
    "            embedding = list(map(float,splitted_line[1:]))\n",
    "            w2emb[word] = embedding\n",
    "        return w2emb\n",
    "\n",
    "    def look_up(self,context, w2emb) :\n",
    "        '''\n",
    "        Remplace dans le vecteur de contexte les mots par leur embedding.\n",
    "\n",
    "        Args:\n",
    "            context (list): liste de taille (size_window*2)+1\n",
    "            w2emb (dictionnary): Associe à chaque mot son embedding\n",
    "\n",
    "        Returns:\n",
    "            list : liste de taille size_embedding : BOW\n",
    "        '''\n",
    "\n",
    "        emb_size = len(list(w2emb.values())[0]) #on récupère la taille d'un embedding : 300\n",
    "        context_emb = np.zeros(emb_size)\n",
    "        for word in context :\n",
    "            if word in w2emb :\n",
    "                context_emb = np.add(context_emb, np.array(w2emb[word]))             \n",
    "        return context_emb\n",
    "    \n",
    "    def select_examples(self,examples,senses,size):\n",
    "        '''\n",
    "        Choisit des examples d'entraînement représentatifs du corpus.\n",
    "\n",
    "        Args:\n",
    "            examples (list)\n",
    "            n_senses (int): nombre de senses associés à l'instance\n",
    "            size (float): quantité des données d'entraînement considérés\n",
    "\n",
    "        Returns:\n",
    "            list: examples qui contiennent au moins un example de chaque sense\n",
    "        '''\n",
    "\n",
    "        selected_examples = []\n",
    "        \n",
    "        #Pour chaque sens, on ajoute un example associé à ce sens ,au hasard\n",
    "        for sense in senses :\n",
    "            selected_examples.append(random.choice(list(filter((lambda example:example[1]==sense),examples))))\n",
    "        \n",
    "        #On calcule ensuite le nombre d'examples qu'il reste à ajouter pour atteindre la quantité de données souhaitée\n",
    "        size_to_add = round(size*(len(examples)))-len(selected_examples)\n",
    "        \n",
    "        #On ajoute ce nombre de données (non-présentes déjà dans la liste) selectionnées au hasard\n",
    "        selected_examples.extend(random.choices(list(filter((lambda example : example not in selected_examples),examples)),k=size_to_add))\n",
    "        \n",
    "        return selected_examples\n",
    "\n",
    "    def classify(self,instance,data_size) :\n",
    "        \n",
    "        clf = MLPClassifier(random_state=1,hidden_layer_sizes=(1000,)) \n",
    "        \n",
    "        selected_examples = self.select_examples(self.w2examples[instance],self.w2senses[instance],data_size)\n",
    "        X = [self.look_up(context,self.w2emb)for context,gold in selected_examples]\n",
    "        y = [gold for context,gold in selected_examples]\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)\n",
    "            \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        print(\"instance :\",instance)\n",
    "        print(f'{data_size*100}% des données annotées considérées')\n",
    "        print(\"nombre de données d'entraînement : \", len(X_train))\n",
    "        print(\"étiquettes possibles pour cette instance : \", self.w2senses[instance])\n",
    "        print(\"étiquettes présentes dans les données d'entraînement :\",Counter(list(zip(*selected_examples))[1]))\n",
    "        print(\"prédiction :\", y_pred)\n",
    "        print(\"gold :\",y_test)\n",
    "        print(\"accuracy score : \", accuracy_score(y_pred,y_test),\"\\n\")\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def test_classifications(self,instances,step):\n",
    "        ''' \n",
    "        Permet d'afficher différents tests de classification pour les instances données. \n",
    "\n",
    "        Args:\n",
    "            instances (list): classifieurs utilisés\n",
    "            step (float): pas de descente de la quantité de données considérées\n",
    "        '''\n",
    "        for i in range(round(1.0/step)):\n",
    "            \n",
    "            data_size = 1.0 - (step*float(i))\n",
    "\n",
    "            for instance in instances :\n",
    "                self.classify(instance,data_size)\n",
    "                print()\n",
    "        return        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassif = Classifieur(data_path,gold_path,embeddings_path)\n",
    "#myClassif.classify(\"aboutir\",0.8)\n",
    "instances = list(myClassif.w2examples.keys())\n",
    "myClassif.test_classifications(instances[:3],0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSD semi-supervisé : K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means():\n",
    "    ''' \n",
    "    classifieur K-means pour un mot particulier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        '''\n",
    "        Instancie les différentes variables utiles pour l'algorithme du K-means\n",
    "\n",
    "        examples : liste d'examples dont le mot à désambiguiser est le même pour \n",
    "                   chaque example\n",
    "        example : couple d'un mot avec son contexte de fenêtre 4 (sous forme \n",
    "                  d'embedding) et du numéro de sens attendu du mot à désambiguiser \n",
    "                  (gold class sous forme d'integer)\n",
    "                    si example = ([1.9, 2.3, 0.6], 1),\n",
    "                    - le contexte avec le mot à désambiguiser et son lemme est \n",
    "                      l'embedding [1.9, 2.3, 0.6]\n",
    "                    - le numéro de sens est 1\n",
    "        '''\n",
    "\n",
    "        # transforme l'ensemble des examples en une liste pour pouvoir garder le \n",
    "        # même indice pour chaque example par la suite\n",
    "        self.examples = list(examples)\n",
    "        # transforme les embeddings en tensors\n",
    "        self.tensors_examples = [example[0] for example in self.examples]\n",
    "        # détermine le nombre de sens possibles k (donc le nombre de clusters) \n",
    "        # à l'aide des données\n",
    "        self.k = self.nb_senses()\n",
    "        # initialisation de centroids : pour chaque sens, un example est pris au hasard\n",
    "        self.tensors_centroids = [random.choice(example) \n",
    "                                  for example in self.examples_of_same_sense().values()]\n",
    "        # initialisation de clusters : tous les examples sont associés au cluster 0\n",
    "        self.clusters = np.zeros(len(examples))\n",
    "\n",
    "    def nb_senses(self):\n",
    "        '''\n",
    "        Renvoie le nombre de sens existants dans un ensemble d'examples\n",
    "        '''\n",
    "\n",
    "        known_senses = []\n",
    "        # pour chaque exemple\n",
    "        for example in self.examples:\n",
    "            # si le sens attendu (gold class) n'a pas encore été rencontré\n",
    "            if example[1] not in known_senses:\n",
    "                # l'ajoute à la liste des sens possibles\n",
    "                known_senses.append(example[1])\n",
    "        # renvoie le nombre de sens\n",
    "        return len(known_senses)\n",
    "    \n",
    "    def examples_of_same_sense(self):\n",
    "        '''\n",
    "        Regroupe les contextes des examples dans un dictionnaire triés selon le \n",
    "        sens du mot à désambiguiser\n",
    "        '''\n",
    "\n",
    "        # clé : numéro du sens\n",
    "        # valeur : liste de contextes avec ce sens en gold class\n",
    "        sense2examples = {}\n",
    "        # pour chaque example\n",
    "        for example in self.examples:\n",
    "            # si sa gold class n'a pas été déjà rencontrée\n",
    "            if example[1] not in sense2examples:\n",
    "                # ajoute une clé pour cette gold class\n",
    "                sense2examples[example[1]] = []\n",
    "            # ajoute le contexte au dictionnaire correspondant au sens utilisé\n",
    "            sense2examples[example[1]].append(example[0])\n",
    "\n",
    "        return sense2examples\n",
    "    \n",
    "    def learn_clusters(self):\n",
    "        '''\n",
    "        Algorithme de K-Means\n",
    "        Retourne les coordonnées de chaque centroide ainsi que le cluster auquel \n",
    "        appartient chaque example\n",
    "        '''\n",
    "\n",
    "        # différence initialisée à Vrai\n",
    "        diff = True\n",
    "        \n",
    "        # tant qu'il y a une différence entre l'ancienne liste et la nouvelle \n",
    "        # liste de centroides\n",
    "        while diff:\n",
    "\n",
    "            # CALCUL DES DISTANCES ENTRE CHAQUE EXAMPLE ET CHAQUE CENTROIDE\n",
    "\n",
    "            # pour chaque couple (indice, coordonnées) dans les examples\n",
    "            for i, tensor_example in enumerate(self.tensors_examples):\n",
    "                # initialisation de la distance minimum à l'infini\n",
    "                min_dist = float('inf')\n",
    "                # pour chaque couple (indice, coordonnées) dans les centroides\n",
    "                for j, tensor_centroid in enumerate(self.tensors_centroids):\n",
    "                    # calcul de la distance entre cet example et ce centroide\n",
    "                    d = 0\n",
    "                    for k in range(len(tensor_example)):\n",
    "                        d += (tensor_centroid[k].item() - tensor_example[k].item())**2\n",
    "                    d = np.sqrt(d)\n",
    "                    # si une distance plus faible est trouvée\n",
    "                    if min_dist > d:\n",
    "                        # la distance ainsi que le centroide sont stockés\n",
    "                        min_dist = d\n",
    "                        self.clusters[i] = j\n",
    "            \n",
    "            # CALCUL DES NOUVEAUX CENTROIDES\n",
    "\n",
    "            # calcul des nouveaux centroides en utilisant le point au milieu de tous les\n",
    "            # autres points du même cluster\n",
    "            new_centroids = pd.DataFrame(self.tensors_examples).groupby(by = self.clusters).mean()\n",
    "            # transforme ces nouveaux centroides en tensors\n",
    "            tensors_new_centroids = []\n",
    "            for i in range(len(new_centroids.index)):\n",
    "                colums = []\n",
    "                for j in range(len(new_centroids.columns)):\n",
    "                    colums.append(int(new_centroids.iat[i,j]))\n",
    "                tensors_new_centroids.append(torch.tensor(colums))\n",
    "\n",
    "            # MISE A JOUR DES CENTROIDES\n",
    "\n",
    "            count_diff = 0\n",
    "            # pour chaque centroide\n",
    "            for i in range(len(self.tensors_centroids)):\n",
    "                # si l'ancien centroide et le nouveau ne sont pas les mêmes\n",
    "                if not(torch.equal(self.tensors_centroids[i], tensors_new_centroids[i])):\n",
    "                    count_diff += 1\n",
    "                    # met à jour le centroide\n",
    "                    self.tensors_centroids = tensors_new_centroids\n",
    "            # s'il n'y a eu aucune différence entre les anciens et les nouveaux centroides, \n",
    "            # la boucle while se termine\n",
    "            if count_diff == 0:\n",
    "                diff = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on écrit encore des trucs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
